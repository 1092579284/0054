#!/usr/bin/env python3
"""
Milanoç½‘ç»œæµé‡é¢„æµ‹ - æ¶ˆèå®éªŒæ¨¡å‹æ¶æ„
=====================================

åŸºäºåŸºçº¿æ¨¡å‹å®ç°å››ç§ä¸åŒçš„æ¨¡å‹æ¶æ„ç”¨äºæ¶ˆèå®éªŒï¼š
1. å•å‘GRUæ¨¡å‹
2. å•å‘GRU+LSTMç»„åˆæ¨¡å‹
3. çº¯LSTMæ¨¡å‹
4. åŸºçº¿æ¨¡å‹(åŒå‘GRU+LSTM)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class UnidirectionalGRUModel(nn.Module):
    """æ¶ˆèå®éªŒ1: åªä½¿ç”¨å•å‘GRU"""
    def __init__(self, input_size, sequence_length):
        super(UnidirectionalGRUModel, self).__init__()
        self.model_name = "å•å‘GRU"
        
        # å•å‘GRUå±‚ (è¾“å‡º64ç»´)
        self.gru = nn.GRU(input_size, 64, batch_first=True, bidirectional=False, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.dense = nn.Linear(64, 64)
        self.dropout = nn.Dropout(0.2)
        self.output = nn.Linear(64, 1)
        
    def forward(self, x):
        # GRUå¤„ç†
        x, _ = self.gru(x)
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        
        # å…¨è¿æ¥å±‚
        x = torch.relu(self.dense(x))
        x = self.dropout(x)
        x = self.output(x)
        return x


class UnidirectionalGRULSTMModel(nn.Module):
    """æ¶ˆèå®éªŒ3: å•å‘GRU + LSTMç»„åˆ"""
    def __init__(self, input_size, sequence_length):
        super(UnidirectionalGRULSTMModel, self).__init__()
        self.model_name = "å•å‘GRU+LSTM"
        
        # å•å‘GRUå±‚ (è¾“å‡º64ç»´)
        self.gru = nn.GRU(input_size, 64, batch_first=True, bidirectional=False, dropout=0.2)
        
        # LSTMå±‚ (64è¾“å…¥ï¼Œ32è¾“å‡º)
        self.lstm = nn.LSTM(64, 32, batch_first=True, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.dense = nn.Linear(32, 64)
        self.dropout = nn.Dropout(0.2)
        self.output = nn.Linear(64, 1)
        
    def forward(self, x):
        # GRUå¤„ç†
        x, _ = self.gru(x)
        
        # LSTMå¤„ç†
        x, _ = self.lstm(x)
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        
        # å…¨è¿æ¥å±‚
        x = torch.relu(self.dense(x))
        x = self.dropout(x)
        x = self.output(x)
        return x


class PureLSTMModel(nn.Module):
    """æ¶ˆèå®éªŒ4: åªä½¿ç”¨LSTM"""
    def __init__(self, input_size, sequence_length):
        super(PureLSTMModel, self).__init__()
        self.model_name = "çº¯LSTM"
        
        # LSTMå±‚ (è¾“å‡º64ç»´)
        self.lstm = nn.LSTM(input_size, 64, batch_first=True, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.dense = nn.Linear(64, 64)
        self.dropout = nn.Dropout(0.2)
        self.output = nn.Linear(64, 1)
        
    def forward(self, x):
        # LSTMå¤„ç†
        x, _ = self.lstm(x)
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        
        # å…¨è¿æ¥å±‚
        x = torch.relu(self.dense(x))
        x = self.dropout(x)
        x = self.output(x)
        return x


class BaselineModel(nn.Module):
    """åŸºçº¿æ¨¡å‹: åŒå‘GRU + LSTM (å‚è€ƒæ¨¡å‹)"""
    def __init__(self, input_size, sequence_length):
        super(BaselineModel, self).__init__()
        self.model_name = "åŸºçº¿æ¨¡å‹(åŒå‘GRU+LSTM)"
        
        # åŒå‘GRUå±‚
        self.gru = nn.GRU(input_size, 64, batch_first=True, bidirectional=True, dropout=0.2)
        
        # LSTMå±‚
        self.lstm = nn.LSTM(128, 32, batch_first=True, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.dense = nn.Linear(32, 64)
        self.dropout = nn.Dropout(0.2)
        self.output = nn.Linear(64, 1)
        
    def forward(self, x):
        # åŒå‘GRUå¤„ç†
        x, _ = self.gru(x)
        
        # LSTMå¤„ç†
        x, _ = self.lstm(x)
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        
        # å…¨è¿æ¥å±‚
        x = torch.relu(self.dense(x))
        x = self.dropout(x)
        x = self.output(x)
        return x


def get_model_by_name(model_name, input_size, sequence_length):
    """æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„æ¨¡å‹å®ä¾‹"""
    models = {
        'unidirectional_gru': UnidirectionalGRUModel,
        'unidirectional_gru_lstm': UnidirectionalGRULSTMModel,
        'pure_lstm': PureLSTMModel,
        'baseline': BaselineModel
    }
    
    if model_name not in models:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹åç§°: {model_name}. å¯é€‰: {list(models.keys())}")
    
    return models[model_name](input_size, sequence_length)


def get_all_model_info():
    """è·å–æ‰€æœ‰æ¨¡å‹çš„ä¿¡æ¯"""
    return {
        'unidirectional_gru': {
            'name': 'å•å‘GRU',
            'description': 'ä½¿ç”¨å•å‘GRUè¿›è¡Œåºåˆ—å»ºæ¨¡',
            'architecture': 'GRU(64) â†’ Dense(64) â†’ Output(1)'
        },
        'unidirectional_gru_lstm': {
            'name': 'å•å‘GRU+LSTM',
            'description': 'å•å‘GRUä¸LSTMçš„çº§è”ç»„åˆ',
            'architecture': 'GRU(64) â†’ LSTM(32) â†’ Dense(64) â†’ Output(1)'
        },
        'pure_lstm': {
            'name': 'çº¯LSTM',
            'description': 'åªä½¿ç”¨LSTMè¿›è¡Œåºåˆ—å»ºæ¨¡',
            'architecture': 'LSTM(64) â†’ Dense(64) â†’ Output(1)'
        },
        'baseline': {
            'name': 'åŸºçº¿æ¨¡å‹',
            'description': 'åŒå‘GRU+LSTMç»„åˆ(åŸå§‹æ¶æ„)',
            'architecture': 'BiGRU(64*2) â†’ LSTM(32) â†’ Dense(64) â†’ Output(1)'
        }
    }


if __name__ == "__main__":
    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    input_size = 13  # åŸºäºæœ€ä½³ç‰¹å¾æ•°é‡
    sequence_length = 8
    batch_size = 32
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("ğŸ§ª æ¶ˆèå®éªŒæ¨¡å‹æ¶æ„æµ‹è¯•")
    print("=" * 50)
    
    model_info = get_all_model_info()
    
    for model_key, info in model_info.items():
        print(f"\nğŸ“‹ {info['name']}")
        print(f"   æè¿°: {info['description']}")
        print(f"   æ¶æ„: {info['architecture']}")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = get_model_by_name(model_key, input_size, sequence_length).to(device)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"   å‚æ•°é‡: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_input = torch.randn(batch_size, sequence_length, input_size).to(device)
        try:
            with torch.no_grad():
                output = model(test_input)
            print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"   âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"   âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    
    print(f"\nğŸ¯ æ‰€æœ‰æ¨¡å‹å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹æ¶ˆèå®éªŒ!")