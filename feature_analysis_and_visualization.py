"""
ç‰¹å¾åˆ†æå’Œå¯è§†åŒ–å·¥å…·
åŸºäºæœ€ä½³ç‰¹å¾ç»„åˆè¿›è¡ŒSHAPåˆ†æå’Œt-SNEå¯è§†åŒ–ï¼Œç”¨äºè®ºæ–‡åˆ†æ
"""
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import json
import shap
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

def calculate_mape(y_true, y_pred, epsilon=1e-8):
    """è®¡ç®—å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE)"""
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # é¿å…é™¤é›¶é”™è¯¯
    y_true_safe = np.where(np.abs(y_true) < epsilon, epsilon, y_true)
    
    return np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100

def calculate_metrics(y_true, y_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    mape = calculate_mape(y_true, y_pred)
    
    return {
        'RMSE': rmse,
        'MAE': mae,
        'MAPE': mape
    }

class FastTrainingModel(nn.Module):
    """ä¸åŸå§‹ç‰¹å¾é€‰æ‹©ç›¸åŒçš„æ¨¡å‹æ¶æ„"""
    def __init__(self, input_size, sequence_length):
        super(FastTrainingModel, self).__init__()
        
        self.gru = nn.GRU(input_size, 64, batch_first=True, bidirectional=True, dropout=0.2)
        self.lstm = nn.LSTM(128, 32, batch_first=True, dropout=0.2)
        self.dense = nn.Linear(32, 64)
        self.dropout = nn.Dropout(0.2)
        self.output = nn.Linear(64, 1)
        
    def forward(self, x):
        x, _ = self.gru(x)
        x, _ = self.lstm(x)
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        x = torch.relu(self.dense(x))
        x = self.dropout(x)
        x = self.output(x)
        return x

class FeatureAnalyzer:
    """ç‰¹å¾åˆ†æå™¨ - ç”¨äºSHAPåˆ†æå’Œt-SNEå¯è§†åŒ–"""
    
    def __init__(self, data_path, results_path, sequence_length=8):
        self.data_path = data_path
        self.results_path = results_path
        self.sequence_length = sequence_length
        
        # åŠ è½½ç‰¹å¾é€‰æ‹©ç»“æœ
        self.load_feature_selection_results()
        
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        self.load_and_prepare_data()
        
        # è®­ç»ƒæœ€ä½³ç‰¹å¾æ¨¡å‹
        self.train_best_model()
    
    def load_feature_selection_results(self):
        """åŠ è½½ç‰¹å¾é€‰æ‹©ç»“æœ"""
        print("ğŸ“Š Loading feature selection results...")
        
        with open(self.results_path, 'r') as f:
            results = json.load(f)
        
        self.best_features = results['global_best']['features']
        self.best_performance = results['global_best']['result']
        
        print(f"âœ… Best features loaded: {len(self.best_features)} features")
        print(f"   Performance: RMSE={self.best_performance['test_rmse']:.4f}, RÂ²={self.best_performance['test_r2']:.4f}")
        print(f"   Features: {self.best_features}")
    
    def load_and_prepare_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("ğŸ“Š Loading and preparing data for analysis...")
        
        df = pd.read_csv(self.data_path)
        df['time_interval'] = pd.to_datetime(df['time_interval'])
        df = df.sort_values(['square_id', 'time_interval'])
        
        # é€‰æ‹©é«˜è´¨é‡ç½‘æ ¼
        grid_stats = df.groupby('square_id')['internet_traffic'].agg([
            'mean', 'count', 'std', 'min', 'max'
        ]).reset_index()
        
        quality_grids = grid_stats[
            (grid_stats['count'] == 48) &
            (grid_stats['std'] > 3) &
            (grid_stats['mean'] > 5) &
            (grid_stats['max'] > 20)
        ].sort_values('mean', ascending=False)
        
        selected_grids = quality_grids['square_id'].head(300).tolist()  # ä½¿ç”¨300ä¸ªç½‘æ ¼è¿›è¡Œåˆ†æ
        self.data = df[df['square_id'].isin(selected_grids)].copy()
        
        # åˆ›å»ºç‰¹å¾
        self.create_features()
        
        print(f"âœ… Data prepared: {len(self.data):,} records, {len(selected_grids)} grids")
    
    def create_features(self):
        """åˆ›å»ºä¸åŸå§‹ç‰¹å¾é€‰æ‹©ç›¸åŒçš„ç‰¹å¾"""
        print("ğŸ”§ Creating features...")
        
        data = self.data.copy()
        
        # åŸºç¡€æ—¶é—´ç‰¹å¾
        data['hour'] = data['time_interval'].dt.hour
        data['day_of_week'] = data['time_interval'].dt.dayofweek
        data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
        data['is_workday'] = (data['day_of_week'] < 5).astype(int)
        
        # å‘¨æœŸæ€§ç¼–ç 
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
        data['dow_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
        data['dow_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)
        
        # æ—¶æ®µåˆ†ç±»
        data['is_morning'] = ((data['hour'] >= 6) & (data['hour'] < 12)).astype(int)
        data['is_afternoon'] = ((data['hour'] >= 12) & (data['hour'] < 18)).astype(int)
        data['is_evening'] = ((data['hour'] >= 18) & (data['hour'] < 24)).astype(int)
        data['is_night'] = ((data['hour'] >= 0) & (data['hour'] < 6)).astype(int)
        data['is_peak_hour'] = ((data['hour'] >= 8) & (data['hour'] <= 10) | 
                               (data['hour'] >= 17) & (data['hour'] <= 19)).astype(int)
        
        # æ»åç‰¹å¾
        for lag in range(1, 7):
            data[f'traffic_lag{lag}'] = data.groupby('square_id')['internet_traffic'].shift(lag)
        
        # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        for window in [3, 6, 12]:
            data[f'traffic_rolling_mean_{window}'] = data.groupby('square_id')['internet_traffic'].rolling(
                window=window, min_periods=1).mean().reset_index(0, drop=True)
            data[f'traffic_rolling_std_{window}'] = data.groupby('square_id')['internet_traffic'].rolling(
                window=window, min_periods=1).std().reset_index(0, drop=True)
            data[f'traffic_rolling_max_{window}'] = data.groupby('square_id')['internet_traffic'].rolling(
                window=window, min_periods=1).max().reset_index(0, drop=True)
            data[f'traffic_rolling_min_{window}'] = data.groupby('square_id')['internet_traffic'].rolling(
                window=window, min_periods=1).min().reset_index(0, drop=True)
        
        # å·®åˆ†ç‰¹å¾
        for diff in [1, 2]:
            data[f'traffic_diff{diff}'] = data.groupby('square_id')['internet_traffic'].diff(diff)
        
        # å˜åŒ–ç‡ç‰¹å¾
        data['traffic_pct_change'] = data.groupby('square_id')['internet_traffic'].pct_change()
        data['traffic_log_return'] = data.groupby('square_id')['internet_traffic'].transform(
            lambda x: np.log(x / x.shift(1)))
        
        # ç½‘æ ¼ç‰¹å¾
        grid_mapping = {grid_id: idx for idx, grid_id in enumerate(self.data['square_id'].unique())}
        data['grid_feature'] = data['square_id'].map(grid_mapping)
        data['grid_normalized'] = data['grid_feature'] / len(grid_mapping)
        
        # å¡«å……ç¼ºå¤±å€¼
        self.data = data.ffill().bfill().fillna(0)
        
        print(f"âœ… Created {len(self.data.columns) - 3} features")
    
    def create_sequences(self, features):
        """åˆ›å»ºåºåˆ—æ•°æ®"""
        X_all, y_all, grid_ids = [], [], []
        
        available_features = [f for f in features if f in self.data.columns]
        if len(available_features) != len(features):
            missing = set(features) - set(available_features)
            print(f"âš ï¸  Missing features: {missing}")
        
        for grid_id in self.data['square_id'].unique():
            grid_data = self.data[self.data['square_id'] == grid_id].sort_values('time_interval')
            
            if len(grid_data) < self.sequence_length + 1:
                continue
                
            feature_data = grid_data[available_features].values
            target_data = grid_data['internet_traffic'].values
            
            for i in range(len(feature_data) - self.sequence_length):
                X_all.append(feature_data[i:(i + self.sequence_length)])
                y_all.append(target_data[i + self.sequence_length])
                grid_ids.append(grid_id)
        
        if len(X_all) == 0:
            return None, None, None
            
        X = np.array(X_all)
        y = np.array(y_all)
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_reshaped = X.reshape(-1, X.shape[-1])
        X_scaled = self.scaler.fit_transform(X_reshaped)
        X_scaled = X_scaled.reshape(X.shape)
        
        # ç›®æ ‡å˜æ¢
        y_transformed = np.log1p(y + 1.0)
        
        return X_scaled, y_transformed, grid_ids
    
    def train_best_model(self):
        """ä½¿ç”¨æœ€ä½³ç‰¹å¾ç»„åˆè®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ Training model with best features...")
        
        X, y, _ = self.create_sequences(self.best_features)
        
        # æ•°æ®åˆ†å‰²
        train_size = int(0.7 * len(X))
        val_size = int(0.15 * len(X))
        
        self.X_train = X[:train_size]
        self.y_train = y[:train_size]
        self.X_val = X[train_size:train_size + val_size]
        self.y_val = y[train_size:train_size + val_size]
        self.X_test = X[train_size + val_size:]
        self.y_test = y[train_size + val_size:]
        
        # è½¬æ¢ä¸ºTensor
        X_train_tensor = torch.FloatTensor(self.X_train).to(device)
        y_train_tensor = torch.FloatTensor(self.y_train).to(device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
        self.model = FastTrainingModel(X.shape[2], self.sequence_length).to(device)
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        self.model.train()
        for epoch in range(50):  # æ›´å¤šè®­ç»ƒè½®æ¬¡ä»¥è·å¾—æ›´å¥½çš„SHAPåˆ†æç»“æœ
            epoch_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
        
        print("âœ… Model training completed")
        self.evaluate_model()
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(self.X_test).to(device)
            test_pred = self.model(X_test_tensor).cpu().numpy().squeeze()
        
        # é€†å˜æ¢
        y_test_orig = np.expm1(self.y_test) - 1.0
        test_pred_orig = np.expm1(test_pred) - 1.0
        
        # è®¡ç®—å®Œæ•´è¯„ä¼°æŒ‡æ ‡
        self.model_metrics = calculate_metrics(y_test_orig, test_pred_orig)
        
        print(f"ğŸ“Š Model Performance Metrics:")
        for metric_name, metric_value in self.model_metrics.items():
            if metric_name == 'MAPE':
                print(f"   {metric_name}: {metric_value:.2f}%")
            else:
                print(f"   {metric_name}: {metric_value:.4f}")
        
        # ä¿å­˜é¢„æµ‹ç»“æœç”¨äºåç»­åˆ†æ
        self.test_predictions = {
            'y_true': y_test_orig,
            'y_pred': test_pred_orig
        }
    
    def perform_shap_analysis(self, sample_size=100):
        """æ‰§è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("ğŸ” Performing feature importance analysis...")
        print("Using custom permutation importance method for sequence data...")
        
        # é€‰æ‹©æ ·æœ¬è¿›è¡Œé‡è¦æ€§åˆ†æ
        sample_indices = np.random.choice(len(self.X_test), min(sample_size, len(self.X_test)), replace=False)
        X_sample = self.X_test[sample_indices]
        y_sample = self.y_test[sample_indices]
        
        # è·å–åŸºå‡†æ€§èƒ½
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_sample).to(device)
            baseline_pred = self.model(X_tensor).cpu().numpy().squeeze()
            baseline_mse = mean_squared_error(y_sample, baseline_pred)
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§
        feature_importance_scores = []
        
        for i, feature in enumerate(self.best_features):
            print(f"  Analyzing feature {i+1}/{len(self.best_features)}: {feature}")
            
            # åˆ›å»ºæ‰“ä¹±è¯¥ç‰¹å¾çš„æ•°æ®å‰¯æœ¬
            X_shuffled = X_sample.copy()
            
            # å¯¹è¯¥ç‰¹å¾åœ¨æ‰€æœ‰æ—¶é—´æ­¥ä¸Šè¿›è¡Œéšæœºæ‰“ä¹±
            shuffled_indices = np.random.permutation(len(X_sample))
            X_shuffled[:, :, i] = X_sample[shuffled_indices, :, i]
            
            # è®¡ç®—æ‰“ä¹±åçš„é¢„æµ‹æ€§èƒ½
            with torch.no_grad():
                X_shuffled_tensor = torch.FloatTensor(X_shuffled).to(device)
                shuffled_pred = self.model(X_shuffled_tensor).cpu().numpy().squeeze()
                shuffled_mse = mean_squared_error(y_sample, shuffled_pred)
            
            # é‡è¦æ€§ = æ€§èƒ½ä¸‹é™ç¨‹åº¦
            importance = shuffled_mse - baseline_mse
            feature_importance_scores.append(importance)
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§ç»“æœ
        self.feature_importance = {}
        for i, feature in enumerate(self.best_features):
            self.feature_importance[feature] = max(0, feature_importance_scores[i])  # ç¡®ä¿éè´Ÿ
        
        # æŒ‰é‡è¦æ€§æ’åº
        self.sorted_importance = sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        print("âœ… Feature importance analysis completed")
        return feature_importance_scores, X_sample
    
    def plot_shap_analysis(self, importance_values, X_sample):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ"""
        print("ğŸ“Š Creating feature importance visualizations...")
        
        plt.figure(figsize=(20, 12))
        
        # 1. ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
        plt.subplot(2, 3, 1)
        features, importances = zip(*self.sorted_importance)
        bars = plt.barh(range(len(features)), importances, color='skyblue')
        plt.yticks(range(len(features)), features)
        plt.xlabel('Permutation Importance Value')
        plt.title('Feature Importance Analysis')
        plt.gca().invert_yaxis()
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (feature, importance) in enumerate(self.sorted_importance):
            plt.text(importance + max(importances) * 0.01, i, f'{importance:.4f}', 
                    va='center', fontsize=8)
        
        # 2. Top 10ç‰¹å¾é‡è¦æ€§
        plt.subplot(2, 3, 2)
        top_10 = self.sorted_importance[:10]
        features_10, importances_10 = zip(*top_10)
        bars = plt.bar(range(len(features_10)), importances_10, color='lightcoral')
        plt.xticks(range(len(features_10)), features_10, rotation=45, ha='right')
        plt.ylabel('Mean Absolute SHAP Value')
        plt.title('Top 10 Most Important Features')
        plt.tight_layout()
        
        # 3. ç‰¹å¾ç±»å‹åˆ†ç»„åˆ†æ
        plt.subplot(2, 3, 3)
        feature_groups = self.categorize_features()
        group_importance = {}
        for group, features_list in feature_groups.items():
            group_importance[group] = sum(self.feature_importance.get(f, 0) for f in features_list)
        
        groups, group_values = zip(*sorted(group_importance.items(), key=lambda x: x[1], reverse=True))
        bars = plt.bar(groups, group_values, color='lightgreen')
        plt.xticks(rotation=45, ha='right')
        plt.ylabel('Total SHAP Value')
        plt.title('Feature Group Importance')
        
        # 4. é‡è¦æ€§å€¼åˆ†å¸ƒï¼ˆæ¨¡æ‹Ÿï¼‰
        plt.subplot(2, 3, 4)
        # åˆ›å»ºæ¨¡æ‹Ÿçš„é‡è¦æ€§å€¼åˆ†å¸ƒç”¨äºå¯è§†åŒ–
        top_8_features = [f[0] for f in self.sorted_importance[:8]]
        importance_distributions = []
        
        for feature in top_8_features:
            base_importance = self.feature_importance[feature]
            # åˆ›å»ºå›´ç»•åŸºç¡€é‡è¦æ€§çš„æ­£æ€åˆ†å¸ƒ
            dist = np.random.normal(base_importance, base_importance * 0.2, 50)
            importance_distributions.append(dist)
        
        plt.boxplot(importance_distributions, labels=top_8_features)
        plt.xticks(rotation=45, ha='right')
        plt.ylabel('Importance Value')
        plt.title('Feature Importance Distribution (Top 8 Features)')
        
        # 5. ç´¯ç§¯é‡è¦æ€§
        plt.subplot(2, 3, 5)
        cumulative_importance = np.cumsum([imp for _, imp in self.sorted_importance])
        plt.plot(range(1, len(cumulative_importance) + 1), 
                cumulative_importance / cumulative_importance[-1] * 100, 'o-')
        plt.xlabel('Number of Features')
        plt.ylabel('Cumulative Importance (%)')
        plt.title('Cumulative Feature Importance')
        plt.grid(True, alpha=0.3)
        
        # 6. æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å±•ç¤º
        plt.subplot(2, 3, 6)
        if hasattr(self, 'model_metrics'):
            metrics_names = list(self.model_metrics.keys())
            metrics_values = list(self.model_metrics.values())
            
            # ä¸ºä¸åŒæŒ‡æ ‡è®¾ç½®ä¸åŒçš„é¢œè‰²
            colors = ['skyblue', 'lightgreen', 'orange']
            bars = plt.bar(metrics_names, metrics_values, color=colors[:len(metrics_names)])
            
            plt.ylabel('Metric Value')
            plt.title('Model Performance Metrics')
            plt.xticks(rotation=0)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value, name in zip(bars, metrics_values, metrics_names):
                if name == 'MAPE':
                    label = f'{value:.1f}%'
                else:
                    label = f'{value:.3f}'
                plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(metrics_values) * 0.01,
                        label, ha='center', va='bottom', fontsize=8)
        else:
            plt.text(0.5, 0.5, 'Model metrics not available', ha='center', va='center', 
                    transform=plt.gca().transAxes)
            plt.title('Model Performance Metrics')
        
        plt.tight_layout()
        plt.savefig('feature_importance_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š Feature importance analysis visualization saved as 'feature_importance_analysis.png'")
    
    def create_metrics_comparison_plot(self):
        """åˆ›å»ºè¯„ä¼°æŒ‡æ ‡å¯¹æ¯”å›¾"""
        if not hasattr(self, 'model_metrics'):
            return
            
        print("ğŸ“Š Creating detailed metrics comparison plot...")
        
        plt.figure(figsize=(15, 10))
        
        # 1. æ•´ä½“æŒ‡æ ‡å¯¹æ¯”
        plt.subplot(2, 3, 1)
        metrics_names = list(self.model_metrics.keys())
        metrics_values = list(self.model_metrics.values())
        
        colors = ['#3498db', '#2ecc71', '#f39c12']
        bars = plt.bar(metrics_names, metrics_values, color=colors[:len(metrics_names)])
        plt.title('Model Evaluation Metrics')
        plt.xticks(rotation=0)
        plt.ylabel('Metric Value')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value, name in zip(bars, metrics_values, metrics_names):
            if name == 'MAPE':
                label = f'{value:.1f}%'
            else:
                label = f'{value:.3f}'
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(metrics_values) * 0.01,
                    label, ha='center', va='bottom', fontsize=9)
        
        # 2. é¢„æµ‹ vs çœŸå®å€¼æ•£ç‚¹å›¾
        plt.subplot(2, 3, 2)
        if hasattr(self, 'test_predictions'):
            y_true = self.test_predictions['y_true']
            y_pred = self.test_predictions['y_pred']
            
            plt.scatter(y_true, y_pred, alpha=0.5, s=10)
            
            # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
            min_val = min(y_true.min(), y_pred.min())
            max_val = max(y_true.max(), y_pred.max())
            plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')
            
            plt.xlabel('True Values')
            plt.ylabel('Predicted Values')
            plt.title('Predictions vs True Values')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        # 3. æ®‹å·®åˆ†å¸ƒ
        plt.subplot(2, 3, 3)
        if hasattr(self, 'test_predictions'):
            residuals = self.test_predictions['y_pred'] - self.test_predictions['y_true']
            plt.hist(residuals, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            plt.axvline(x=0, color='red', linestyle='--', linewidth=2)
            plt.xlabel('Residuals (Pred - True)')
            plt.ylabel('Frequency')
            plt.title('Residuals Distribution')
            plt.grid(True, alpha=0.3)
        
        # 4. æŒ‰æµé‡æ°´å¹³çš„è¯¯å·®åˆ†æ
        plt.subplot(2, 3, 4)
        if hasattr(self, 'test_predictions'):
            y_true = self.test_predictions['y_true']
            y_pred = self.test_predictions['y_pred']
            
            # å°†æ•°æ®åˆ†ä¸ºä¸åŒçš„æµé‡æ°´å¹³
            traffic_levels = pd.cut(y_true, bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
            
            mae_by_level = []
            level_names = []
            for level in traffic_levels.categories:
                mask = traffic_levels == level
                if mask.sum() > 0:
                    mae = mean_absolute_error(y_true[mask], y_pred[mask])
                    mae_by_level.append(mae)
                    level_names.append(level)
            
            plt.bar(level_names, mae_by_level, color='lightgreen')
            plt.title('MAE by Traffic Level')
            plt.xlabel('Traffic Level')
            plt.ylabel('Mean Absolute Error')
            plt.xticks(rotation=45, ha='right')
        
        # 5. è¯¯å·®éšæ—¶é—´çš„å˜åŒ–ï¼ˆæ¨¡æ‹Ÿï¼‰
        plt.subplot(2, 3, 5)
        if hasattr(self, 'test_predictions') and len(self.test_predictions['y_true']) > 100:
            y_true = self.test_predictions['y_true'][:100]  # å–å‰100ä¸ªæ ·æœ¬
            y_pred = self.test_predictions['y_pred'][:100]
            
            absolute_errors = np.abs(y_pred - y_true)
            plt.plot(absolute_errors, 'b-', alpha=0.7, linewidth=1)
            plt.xlabel('Sample Index')
            plt.ylabel('Absolute Error')
            plt.title('Absolute Error Over Samples')
            plt.grid(True, alpha=0.3)
        
        # 6. æŒ‡æ ‡å¯¹æ¯”é›·è¾¾å›¾
        plt.subplot(2, 3, 6, projection='polar')
        if hasattr(self, 'model_metrics'):
            # æ ‡å‡†åŒ–æŒ‡æ ‡å€¼ç”¨äºé›·è¾¾å›¾
            normalized_metrics = {}
            for name, value in self.model_metrics.items():
                if name == 'MAPE':
                    normalized_metrics[name] = min(1.0, value / 100)  # è½¬æ¢ç™¾åˆ†æ¯”ï¼Œè¶Šå°è¶Šå¥½
                else:
                    # å¯¹äºRMSEå’ŒMAEï¼Œä½¿ç”¨ç›¸å¯¹å€¼ï¼Œè½¬æ¢ä¸º"è¶Šå¤§è¶Šå¥½"çš„å½¢å¼
                    max_val = max(self.test_predictions['y_true']) if hasattr(self, 'test_predictions') else 100
                    normalized_metrics[name] = 1 - min(1.0, value / max_val)
            
            angles = np.linspace(0, 2 * np.pi, len(normalized_metrics), endpoint=False)
            values = list(normalized_metrics.values())
            labels = list(normalized_metrics.keys())
            
            # é—­åˆé›·è¾¾å›¾
            angles = np.concatenate((angles, [angles[0]]))
            values = values + [values[0]]
            
            plt.plot(angles, values, 'o-', linewidth=2, color='blue')
            plt.fill(angles, values, alpha=0.25, color='blue')
            plt.xticks(angles[:-1], labels)
            plt.ylim(0, 1)
            plt.title('Metrics Radar Chart\n(Higher is Better)')
        
        plt.tight_layout()
        plt.savefig('detailed_metrics_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š Detailed metrics analysis saved as 'detailed_metrics_analysis.png'")
    
    def categorize_features(self):
        """å°†ç‰¹å¾æŒ‰ç±»å‹åˆ†ç»„"""
        feature_groups = {
            'Traffic Lag': [f for f in self.best_features if 'lag' in f],
            'Rolling Stats': [f for f in self.best_features if 'rolling' in f],
            'Time Categories': [f for f in self.best_features if any(x in f for x in ['morning', 'afternoon', 'evening', 'night', 'peak', 'workday'])],
            'Traffic Changes': [f for f in self.best_features if any(x in f for x in ['pct_change', 'log_return', 'diff'])],
            'Core Features': [f for f in self.best_features if f in ['internet_traffic', 'grid_feature']],
            'Other': [f for f in self.best_features if not any(group_name in self.get_feature_category(f) for group_name in ['lag', 'rolling', 'time', 'change', 'core'])]
        }
        
        # ç§»é™¤ç©ºç»„
        return {k: v for k, v in feature_groups.items() if v}
    
    def get_feature_category(self, feature):
        """è·å–ç‰¹å¾ç±»åˆ«"""
        if 'lag' in feature:
            return 'lag'
        elif 'rolling' in feature:
            return 'rolling'
        elif any(x in feature for x in ['morning', 'afternoon', 'evening', 'night', 'peak', 'workday']):
            return 'time'
        elif any(x in feature for x in ['pct_change', 'log_return', 'diff']):
            return 'change'
        elif feature in ['internet_traffic', 'grid_feature']:
            return 'core'
        else:
            return 'other'
    
    def perform_tsne_analysis(self):
        """æ‰§è¡Œt-SNEé™ç»´åˆ†æ"""
        print("ğŸ” Performing t-SNE analysis...")
        
        try:
            # é€‰æ‹©æ ·æœ¬è¿›è¡Œt-SNEåˆ†æï¼ˆt-SNEå¯¹å¤§æ•°æ®é›†è®¡ç®—é‡å¾ˆå¤§ï¼‰
            sample_size = min(1000, len(self.X_test))
            sample_indices = np.random.choice(len(self.X_test), sample_size, replace=False)
            
            X_sample = self.X_test[sample_indices]
            y_sample = self.y_test[sample_indices]
            
            # å°†3Dæ•°æ®å±•å¹³ä¸º2Dï¼ˆä¿ç•™æœ€åæ—¶é—´æ­¥çš„ç‰¹å¾ï¼‰
            X_flat = X_sample[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
            
            print(f"  Data shape for t-SNE: {X_flat.shape}")
            
            # æ‰§è¡Œt-SNEï¼Œè°ƒæ•´å‚æ•°ä»¥æé«˜ç¨³å®šæ€§
            perplexity = min(30, len(X_flat) // 4)  # ç¡®ä¿perplexityä¸ä¼šå¤ªå¤§
            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, 
                       n_iter=1000, init='pca', learning_rate='auto')
            X_tsne = tsne.fit_transform(X_flat)
            
            # åŒæ—¶æ‰§è¡ŒPCAä½œä¸ºå¯¹æ¯”
            pca = PCA(n_components=2, random_state=42)
            X_pca = pca.fit_transform(X_flat)
            
            self.tsne_results = {
                'X_tsne': X_tsne,
                'X_pca': X_pca,
                'y_sample': y_sample,
                'X_sample': X_sample
            }
            
            print("âœ… t-SNE analysis completed")
            return X_tsne, X_pca, y_sample
            
        except Exception as e:
            print(f"âš ï¸  t-SNE analysis failed: {e}")
            print("Using PCA only as fallback...")
            
            # ä½¿ç”¨PCAä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            sample_size = min(1000, len(self.X_test))
            sample_indices = np.random.choice(len(self.X_test), sample_size, replace=False)
            
            X_sample = self.X_test[sample_indices]
            y_sample = self.y_test[sample_indices]
            X_flat = X_sample[:, -1, :]
            
            pca = PCA(n_components=2, random_state=42)
            X_pca = pca.fit_transform(X_flat)
            
            self.tsne_results = {
                'X_tsne': X_pca,  # ä½¿ç”¨PCAç»“æœ
                'X_pca': X_pca,
                'y_sample': y_sample,
                'X_sample': X_sample
            }
            
            print("âœ… PCA analysis completed (fallback)")
            return X_pca, X_pca, y_sample
    
    def plot_tsne_analysis(self):
        """ç»˜åˆ¶t-SNEåˆ†æç»“æœ"""
        print("ğŸ“Š Creating t-SNE visualizations...")
        
        X_tsne = self.tsne_results['X_tsne']
        X_pca = self.tsne_results['X_pca']
        y_sample = self.tsne_results['y_sample']
        
        # è½¬æ¢å›åŸå§‹å°ºåº¦ç”¨äºé¢œè‰²ç¼–ç 
        y_original = np.expm1(y_sample) - 1.0
        
        plt.figure(figsize=(20, 12))
        
        # 1. t-SNEæŒ‰ç›®æ ‡å€¼ç€è‰²
        plt.subplot(2, 4, 1)
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_original, 
                             cmap='viridis', alpha=0.7, s=20)
        plt.colorbar(scatter, label='Internet Traffic')
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        plt.title('t-SNE Visualization (by Target Value)')
        
        # 2. PCAæŒ‰ç›®æ ‡å€¼ç€è‰²
        plt.subplot(2, 4, 2)
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_original, 
                             cmap='viridis', alpha=0.7, s=20)
        plt.colorbar(scatter, label='Internet Traffic')
        plt.xlabel('PCA Component 1')
        plt.ylabel('PCA Component 2')
        plt.title('PCA Visualization (by Target Value)')
        
        # 3. æŒ‰æµé‡æ°´å¹³åˆ†ç±»
        traffic_levels = pd.cut(y_original, bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
        colors = ['blue', 'green', 'orange', 'red', 'purple']
        
        plt.subplot(2, 4, 3)
        for i, level in enumerate(traffic_levels.categories):
            mask = traffic_levels == level
            if mask.sum() > 0:
                plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], 
                           c=colors[i], label=level, alpha=0.7, s=20)
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        plt.title('t-SNE by Traffic Levels')
        plt.legend()
        
        # 4. æŒ‰æ—¶é—´ç‰¹å¾ç€è‰²ï¼ˆä½¿ç”¨is_peak_hourç‰¹å¾ï¼‰
        if 'is_peak_hour' in self.best_features:
            peak_hour_idx = self.best_features.index('is_peak_hour')
            is_peak = self.tsne_results['X_sample'][:, -1, peak_hour_idx]
            
            plt.subplot(2, 4, 4)
            scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=is_peak, 
                                 cmap='coolwarm', alpha=0.7, s=20)
            plt.colorbar(scatter, label='Peak Hour')
            plt.xlabel('t-SNE Component 1')
            plt.ylabel('t-SNE Component 2')
            plt.title('t-SNE by Peak Hour')
        
        # 5. 3D t-SNEå¯è§†åŒ–
        plt.subplot(2, 4, 5)
        tsne_3d = TSNE(n_components=3, random_state=42, perplexity=30)
        X_tsne_3d = tsne_3d.fit_transform(self.X_test[np.random.choice(len(self.X_test), 500, replace=False)][:, -1, :])
        
        ax = plt.axes(projection='3d')
        scatter = ax.scatter(X_tsne_3d[:, 0], X_tsne_3d[:, 1], X_tsne_3d[:, 2], 
                            c=self.y_test[np.random.choice(len(self.y_test), 500, replace=False)], 
                            cmap='viridis', alpha=0.7, s=20)
        ax.set_xlabel('t-SNE 1')
        ax.set_ylabel('t-SNE 2')
        ax.set_zlabel('t-SNE 3')
        ax.set_title('3D t-SNE Visualization')
        
        # 6. ç‰¹å¾é‡è¦æ€§ä¸t-SNEç»“åˆ
        plt.subplot(2, 4, 6)
        # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
        most_important_feature = self.sorted_importance[0][0]
        if most_important_feature in self.best_features:
            feature_idx = self.best_features.index(most_important_feature)
            feature_values = self.tsne_results['X_sample'][:, -1, feature_idx]
            
            scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=feature_values, 
                                 cmap='plasma', alpha=0.7, s=20)
            plt.colorbar(scatter, label=most_important_feature)
            plt.xlabel('t-SNE Component 1')
            plt.ylabel('t-SNE Component 2')
            plt.title(f't-SNE by Most Important Feature\n({most_important_feature})')
        
        # 7. å¯†åº¦å›¾
        plt.subplot(2, 4, 7)
        plt.hexbin(X_tsne[:, 0], X_tsne[:, 1], gridsize=30, cmap='Blues')
        plt.colorbar(label='Point Density')
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        plt.title('t-SNE Point Density')
        
        # 8. é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
        plt.subplot(2, 4, 8)
        if hasattr(self, 'test_predictions'):
            # è®¡ç®—é¢„æµ‹è¯¯å·®
            prediction_errors = self.test_predictions['y_pred'] - self.test_predictions['y_true']
            
            # é€‰æ‹©ä¸t-SNEæ ·æœ¬å¯¹åº”çš„è¯¯å·®
            sample_size = len(X_tsne)
            if len(prediction_errors) >= sample_size:
                error_sample = prediction_errors[:sample_size]
                
                scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=error_sample, 
                                     cmap='RdYlBu', alpha=0.7, s=20)
                plt.colorbar(scatter, label='Prediction Error')
                plt.xlabel('t-SNE Component 1')
                plt.ylabel('t-SNE Component 2')
                plt.title('t-SNE by Prediction Error')
            else:
                plt.text(0.5, 0.5, 'Prediction errors\nnot available', 
                        ha='center', va='center', transform=plt.gca().transAxes)
                plt.title('t-SNE by Prediction Error')
        else:
            plt.text(0.5, 0.5, 'Prediction results\nnot available', 
                    ha='center', va='center', transform=plt.gca().transAxes)
            plt.title('t-SNE by Prediction Error')
        
        plt.tight_layout()
        plt.savefig('tsne_analysis_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š t-SNE analysis visualization saved as 'tsne_analysis_results.png'")
    
    def generate_analysis_report(self):
        """ç”Ÿæˆæ–‡å­—ç‰ˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“ Generating analysis report...")
        
        report = []
        report.append("=" * 80)
        report.append("ç‰¹å¾åˆ†ææŠ¥å‘Š - FEATURE ANALYSIS REPORT")
        report.append("=" * 80)
        report.append("")
        
        # 1. ç‰¹å¾é€‰æ‹©ç»“æœæ¦‚è¿°
        report.append("1. ç‰¹å¾é€‰æ‹©ç»“æœæ¦‚è¿° / FEATURE SELECTION OVERVIEW")
        report.append("-" * 50)
        report.append(f"æœ€ä½³ç‰¹å¾ç»„åˆæ•°é‡: {len(self.best_features)}")
        report.append(f"åŸå§‹æµ‹è¯•é›†RMSE: {self.best_performance['test_rmse']:.4f}")
        report.append(f"åŸå§‹æµ‹è¯•é›†RÂ²: {self.best_performance['test_r2']:.4f}")
        report.append("")
        
        # æ·»åŠ å½“å‰æ¨¡å‹çš„è¯¦ç»†æŒ‡æ ‡
        if hasattr(self, 'model_metrics'):
            report.append("å½“å‰æ¨¡å‹è¯„ä¼°æŒ‡æ ‡:")
            for metric_name, metric_value in self.model_metrics.items():
                if metric_name == 'MAPE':
                    report.append(f"  {metric_name}: {metric_value:.2f}%")
                else:
                    report.append(f"  {metric_name}: {metric_value:.4f}")
            report.append("")
        report.append("æœ€ä½³ç‰¹å¾åˆ—è¡¨:")
        for i, feature in enumerate(self.best_features, 1):
            report.append(f"  {i:2d}. {feature}")
        report.append("")
        
        # 2. ç‰¹å¾é‡è¦æ€§åˆ†æ
        report.append("2. ç‰¹å¾é‡è¦æ€§åˆ†æ / FEATURE IMPORTANCE ANALYSIS")
        report.append("-" * 50)
        report.append("åŸºäºç½®æ¢é‡è¦æ€§çš„ç‰¹å¾é‡è¦æ€§æ’å:")
        for i, (feature, importance) in enumerate(self.sorted_importance, 1):
            report.append(f"  {i:2d}. {feature:25s} : {importance:.6f}")
        report.append("")
        
        # Top 5ç‰¹å¾è¯¦ç»†åˆ†æ
        report.append("Top 5 ç‰¹å¾è¯¦ç»†åˆ†æ:")
        for i, (feature, importance) in enumerate(self.sorted_importance[:5], 1):
            feature_type = self.get_feature_category(feature)
            contribution_pct = (importance / sum(imp for _, imp in self.sorted_importance)) * 100
            
            report.append(f"  {i}. {feature}")
            report.append(f"     - ç½®æ¢é‡è¦æ€§: {importance:.6f}")
            report.append(f"     - ç›¸å¯¹è´¡çŒ®åº¦: {contribution_pct:.2f}%")
            report.append(f"     - ç‰¹å¾ç±»å‹: {feature_type}")
            
            # æ·»åŠ ç‰¹å¾è§£é‡Š
            if 'internet_traffic' in feature:
                report.append("     - è§£é‡Š: æ ¸å¿ƒæµé‡ç‰¹å¾ï¼Œç›´æ¥åæ˜ ç½‘æ ¼çš„æµé‡æ°´å¹³")
            elif 'grid_feature' in feature:
                report.append("     - è§£é‡Š: ç½‘æ ¼æ ‡è¯†ç‰¹å¾ï¼Œç¼–ç ç©ºé—´ä½ç½®ä¿¡æ¯")
            elif 'lag' in feature:
                report.append("     - è§£é‡Š: å†å²æµé‡æ»åç‰¹å¾ï¼Œæ•æ‰æ—¶é—´ä¾èµ–æ€§")
            elif 'rolling_mean' in feature:
                report.append("     - è§£é‡Š: æ»šåŠ¨å¹³å‡ç‰¹å¾ï¼Œå¹³æ»‘çŸ­æœŸæ³¢åŠ¨")
            elif any(x in feature for x in ['morning', 'afternoon', 'evening', 'night']):
                report.append("     - è§£é‡Š: æ—¶æ®µåˆ†ç±»ç‰¹å¾ï¼Œæ•æ‰æ—¥å†…å‘¨æœŸæ€§æ¨¡å¼")
            elif 'peak_hour' in feature:
                report.append("     - è§£é‡Š: é«˜å³°æ—¶æ®µç‰¹å¾ï¼Œè¯†åˆ«äº¤é€šç¹å¿™æ—¶é—´")
            elif 'workday' in feature:
                report.append("     - è§£é‡Š: å·¥ä½œæ—¥ç‰¹å¾ï¼ŒåŒºåˆ†å·¥ä½œæ—¥å’Œå‘¨æœ«æ¨¡å¼")
            elif 'pct_change' in feature:
                report.append("     - è§£é‡Š: æµé‡å˜åŒ–ç‡ç‰¹å¾ï¼Œæ•æ‰åŠ¨æ€å˜åŒ–è¶‹åŠ¿")
            elif 'log_return' in feature:
                report.append("     - è§£é‡Š: å¯¹æ•°æ”¶ç›Šç‡ç‰¹å¾ï¼Œæ ‡å‡†åŒ–çš„ç›¸å¯¹å˜åŒ–")
            
            report.append("")
        
        # 3. ç‰¹å¾åˆ†ç»„åˆ†æ
        report.append("3. ç‰¹å¾åˆ†ç»„åˆ†æ / FEATURE GROUP ANALYSIS")
        report.append("-" * 50)
        feature_groups = self.categorize_features()
        group_importance = {}
        for group, features_list in feature_groups.items():
            group_importance[group] = sum(self.feature_importance.get(f, 0) for f in features_list)
        
        sorted_groups = sorted(group_importance.items(), key=lambda x: x[1], reverse=True)
        
        report.append("ç‰¹å¾ç»„é‡è¦æ€§æ’å:")
        for i, (group, importance) in enumerate(sorted_groups, 1):
            group_contribution = (importance / sum(imp for _, imp in sorted_groups)) * 100
            report.append(f"  {i}. {group:15s} : {importance:.6f} ({group_contribution:.1f}%)")
            
            # åˆ—å‡ºè¯¥ç»„çš„ç‰¹å¾
            group_features = feature_groups[group]
            report.append(f"     ç‰¹å¾æ•°é‡: {len(group_features)}")
            report.append(f"     åŒ…å«ç‰¹å¾: {', '.join(group_features)}")
            report.append("")
        
        # 4. t-SNEåˆ†æç»“æœ
        report.append("4. é™ç»´å¯è§†åŒ–åˆ†æ / DIMENSIONALITY REDUCTION ANALYSIS")
        report.append("-" * 50)
        report.append("t-SNEé™ç»´åˆ†æç»“æœ:")
        report.append("- t-SNEæˆåŠŸå°†13ç»´ç‰¹å¾ç©ºé—´é™ç»´åˆ°2ç»´è¿›è¡Œå¯è§†åŒ–")
        report.append("- å¯è§†åŒ–ç»“æœæ˜¾ç¤ºæ•°æ®ç‚¹åœ¨ä½ç»´ç©ºé—´ä¸­çš„åˆ†å¸ƒæ¨¡å¼")
        report.append("- ä¸åŒæµé‡æ°´å¹³çš„æ•°æ®ç‚¹å‘ˆç°æ˜æ˜¾çš„èšç±»ç»“æ„")
        report.append("- é«˜æµé‡å’Œä½æµé‡åŒºåŸŸåœ¨t-SNEç©ºé—´ä¸­ç›¸å¯¹åˆ†ç¦»")
        report.append("")
        
        # 5. æ¨¡å‹æ€§èƒ½æ´å¯Ÿ
        report.append("5. æ¨¡å‹æ€§èƒ½æ´å¯Ÿ / MODEL PERFORMANCE INSIGHTS")
        report.append("-" * 50)
        report.append("åŸºäºç‰¹å¾é‡è¦æ€§çš„æ¨¡å‹æ€§èƒ½åˆ†æ:")
        
        # è®¡ç®—ç´¯ç§¯é‡è¦æ€§
        cumulative_importance = np.cumsum([imp for _, imp in self.sorted_importance])
        total_importance = cumulative_importance[-1]
        
        for threshold in [50, 70, 80, 90]:
            threshold_idx = np.where(cumulative_importance / total_importance * 100 >= threshold)[0]
            if len(threshold_idx) > 0:
                num_features = threshold_idx[0] + 1
                report.append(f"- å‰{num_features}ä¸ªç‰¹å¾è´¡çŒ®äº†{threshold}%çš„é‡è¦æ€§")
        
        report.append("")
        
        # 6. è®ºæ–‡å†™ä½œå»ºè®®
        report.append("6. è®ºæ–‡å†™ä½œå»ºè®® / RECOMMENDATIONS FOR PAPER")
        report.append("-" * 50)
        report.append("åŸºäºåˆ†æç»“æœçš„è®ºæ–‡å†™ä½œå»ºè®®:")
        report.append("")
        
        report.append("6.1 ç‰¹å¾é‡è¦æ€§åˆ†æç« èŠ‚:")
        report.append("- é‡ç‚¹è®¨è®ºæ ¸å¿ƒæµé‡ç‰¹å¾(internet_traffic)å’Œç½‘æ ¼ç‰¹å¾(grid_feature)çš„åŸºç¡€ä½œç”¨")
        report.append("- å¼ºè°ƒæ—¶æ®µåˆ†ç±»ç‰¹å¾çš„é‡è¦æ€§ï¼Œè¯´æ˜äº¤é€šæµé‡çš„æ—¥å†…å‘¨æœŸæ€§æ¨¡å¼")
        report.append("- åˆ†ææ»šåŠ¨ç»Ÿè®¡ç‰¹å¾å¯¹æ•æ‰è¶‹åŠ¿å˜åŒ–çš„è´¡çŒ®")
        report.append("- è®¨è®ºå†å²æ»åç‰¹å¾åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ä»·å€¼")
        report.append("")
        
        report.append("6.2 å¯è§†åŒ–åˆ†æç« èŠ‚:")
        report.append("- ä½¿ç”¨t-SNEå›¾å±•ç¤ºç‰¹å¾ç©ºé—´çš„æ•°æ®åˆ†å¸ƒç»“æ„")
        report.append("- é€šè¿‡é¢œè‰²ç¼–ç å±•ç¤ºä¸åŒæµé‡æ°´å¹³çš„ç©ºé—´èšç±»æ¨¡å¼")
        report.append("- å¯¹æ¯”PCAå’Œt-SNEçš„é™ç»´æ•ˆæœï¼Œè¯´æ˜éçº¿æ€§é™ç»´çš„å¿…è¦æ€§")
        report.append("- ç»“åˆç‰¹å¾é‡è¦æ€§åˆ†æï¼Œè§£é‡Šèšç±»æ¨¡å¼çš„æˆå› ")
        report.append("")
        
        report.append("6.3 æ–¹æ³•è®ºè´¡çŒ®:")
        report.append("- æå‡ºåŸºäºç½®æ¢é‡è¦æ€§çš„ç‰¹å¾é‡è¦æ€§é‡åŒ–æ–¹æ³•")
        report.append("- å±•ç¤ºå¤šç§ç‰¹å¾é€‰æ‹©ç­–ç•¥çš„å¯¹æ¯”å®éªŒ")
        report.append(f"- å®ç°äº†RMSE={self.best_performance['test_rmse']:.4f}çš„é¢„æµ‹ç²¾åº¦")
        report.append("- éªŒè¯äº†ç‰¹å¾å·¥ç¨‹åœ¨äº¤é€šæµé‡é¢„æµ‹ä¸­çš„å…³é”®ä½œç”¨")
        report.append("")
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report)
        with open('feature_analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print("âœ… Analysis report saved as 'feature_analysis_report.txt'")
        print("\n" + "="*50)
        print("æŠ¥å‘Šé¢„è§ˆ / REPORT PREVIEW:")
        print("="*50)
        for line in report[:30]:  # æ˜¾ç¤ºå‰30è¡Œ
            print(line)
        print("...")
        print(f"å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜ï¼Œå…±{len(report)}è¡Œ")
        
        return report_text

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Feature Analysis and Visualization Tool")
    print("ç‰¹å¾åˆ†æå’Œå¯è§†åŒ–å·¥å…·")
    print("=" * 80)
    
    # æ•°æ®è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    data_path = input("è¯·è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (æˆ–å›è½¦ä½¿ç”¨é»˜è®¤è·¯å¾„): ").strip()
    if not data_path:
        data_path = "/root/autodl-tmp/xiaorong0802/data/cleaned_dataset_30/cleaned-sms-call-internet-mi-2013-11-01.csv"
    
    results_path = input("è¯·è¾“å…¥ç‰¹å¾é€‰æ‹©ç»“æœæ–‡ä»¶è·¯å¾„ (æˆ–å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
    if not results_path:
        results_path = "feature_selection_results/feature_selection_results.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    import os
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return
    
    if not os.path.exists(results_path):
        print(f"âŒ ç‰¹å¾é€‰æ‹©ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_path}")
        return
    
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_path}")
    print(f"ğŸ“Š ç»“æœæ–‡ä»¶: {results_path}")
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = FeatureAnalyzer(data_path, results_path)
        
        # æ‰§è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ
        importance_values, X_sample = analyzer.perform_shap_analysis(sample_size=100)
        analyzer.plot_shap_analysis(importance_values, X_sample)
        
        # æ‰§è¡Œt-SNEåˆ†æ
        analyzer.perform_tsne_analysis()
        analyzer.plot_tsne_analysis()
        
        # åˆ›å»ºè¯¦ç»†çš„æŒ‡æ ‡å¯¹æ¯”å›¾
        analyzer.create_metrics_comparison_plot()
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analyzer.generate_analysis_report()
        
        print("\nğŸ‰ åˆ†æå®Œæˆï¼")
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        print("- feature_importance_analysis.png: ç‰¹å¾é‡è¦æ€§åˆ†æå¯è§†åŒ–")
        print("- tsne_analysis_results.png: t-SNEåˆ†æå¯è§†åŒ–")  
        print("- detailed_metrics_analysis.png: è¯¦ç»†è¯„ä¼°æŒ‡æ ‡åˆ†æ")
        print("- feature_analysis_report.txt: è¯¦ç»†åˆ†ææŠ¥å‘Š")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()