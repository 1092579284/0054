#!/usr/bin/env python3
"""
Milanoç½‘ç»œæµé‡é¢„æµ‹ - æ•°æ®é‡å¯¹æ¯”å®éªŒï¼ˆä¿®å¤ç‰ˆï¼‰
==================================

åŸºäºåŸºçº¿æ¨¡å‹ç ”ç©¶ä¸åŒè®­ç»ƒæ•°æ®é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
å›ç­”"æœ€ä½³è¾“å…¥æ•°æ®é‡æ˜¯å¤šå°‘"è¿™ä¸ªç ”ç©¶é—®é¢˜
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
import json
from datetime import datetime
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥åŸºçº¿æ¨¡å‹ä»£ç 
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# å¯¼å…¥åŸºçº¿æ¨¡å‹çš„æ•°æ®å¤„ç†ç±»
try:
    from optimal_features_training.milano_optimal_features_corrected import CorrectedOptimalFeaturesMilanoPredictor
    print("âœ… æˆåŠŸå¯¼å…¥åŸºçº¿æ¨¡å‹")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿è¿è¡Œç¯å¢ƒåŒ…å«å¿…è¦çš„ä¾èµ–")
    sys.exit(1)

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)

# GPUè®¾ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æ•°æ®è·¯å¾„
DATA_PATH = 'data/cleaned_dataset_30/cleaned-sms-call-internet-mi-2013-11-01.csv'

class BaselineModel(nn.Module):
    """åŸºçº¿æ¨¡å‹: åŒå‘GRU + LSTM (ä¸æ¶ˆèå®éªŒä¿æŒä¸€è‡´)"""
    def __init__(self, input_size, sequence_length):
        super(BaselineModel, self).__init__()
        self.model_name = "åŸºçº¿æ¨¡å‹(åŒå‘GRU+LSTM)"
        
        # åŒå‘GRUå±‚
        self.gru = nn.GRU(input_size, 64, batch_first=True, bidirectional=True, dropout=0.2)
        
        # LSTMå±‚
        self.lstm = nn.LSTM(128, 32, batch_first=True, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.dense = nn.Linear(32, 64)
        self.dropout = nn.Dropout(0.2)
        self.output = nn.Linear(64, 1)
        
    def forward(self, x):
        # åŒå‘GRUå¤„ç†
        x, _ = self.gru(x)
        
        # LSTMå¤„ç†
        x, _ = self.lstm(x)
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        
        # å…¨è¿æ¥å±‚
        x = torch.relu(self.dense(x))
        x = self.dropout(x)
        x = self.output(x)
        return x


class DataScalingExperiment:
    """æ•°æ®é‡å¯¹æ¯”å®éªŒç±»"""
    
    def __init__(self, sequence_length=8):
        """
        åˆå§‹åŒ–æ•°æ®é‡å¯¹æ¯”å®éªŒ
        
        Args:
            sequence_length: åºåˆ—é•¿åº¦
        """
        self.sequence_length = sequence_length
        self.results = {}
        
    def load_and_prepare_data_with_grids(self, max_grids):
        """
        åŠ è½½å’Œå‡†å¤‡æ•°æ®ï¼ˆæŒ‡å®šç½‘æ ¼æ•°é‡ï¼‰
        
        Args:
            max_grids: æœ€å¤§ç½‘æ ¼æ•°é‡
            
        Returns:
            tuple: (X, y) åºåˆ—æ•°æ®
        """
        print(f"ğŸ“¥ åŠ è½½æ•°æ® (æœ€å¤§ç½‘æ ¼æ•°: {max_grids})...")
        
        # ä½¿ç”¨åŸºçº¿æ¨¡å‹çš„æ•°æ®å¤„ç†å™¨
        data_processor = CorrectedOptimalFeaturesMilanoPredictor(
            sequence_length=self.sequence_length, 
            max_grids=max_grids
        )
        
        # åŠ è½½æ•°æ®
        data, selected_grids = data_processor.load_and_preprocess_data()
        
        # åˆ›å»ºåºåˆ—æ•°æ®
        features = data_processor.optimal_features
        X, y = data_processor.create_sequences(features)
        
        if X is None or len(X) < 100:
            print(f"âš ï¸  ç½‘æ ¼æ•° {max_grids} çš„æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡...")
            return None, None
        
        print(f"   æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        print(f"   å®é™…ç½‘æ ¼æ•°: {len(selected_grids)}")
        print(f"   æ ·æœ¬æ€»æ•°: {len(X):,}")
        
        return X, y
    
    def train_single_model(self, X, y, data_size_name, epochs=30, batch_size=32):
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å€¼
            data_size_name: æ•°æ®è§„æ¨¡åç§°
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            dict: è®­ç»ƒå’Œè¯„ä¼°ç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ: {data_size_name}")
        print(f"   æ ·æœ¬æ•°é‡: {len(X):,}")
        
        # æ•°æ®åˆ†å‰² (70%-15%-15%)
        train_size = int(0.7 * len(X))
        val_size = int(0.15 * len(X))
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size + val_size]
        y_val = y[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]
        y_test = y[train_size + val_size:]
        
        print(f"   è®­ç»ƒé›†: {len(X_train):,}, éªŒè¯é›†: {len(X_val):,}, æµ‹è¯•é›†: {len(X_test):,}")
        
        # è½¬æ¢ä¸ºTensor
        X_train_tensor = torch.FloatTensor(X_train).to(device)
        y_train_tensor = torch.FloatTensor(y_train).to(device)
        X_val_tensor = torch.FloatTensor(X_val).to(device)
        y_val_tensor = torch.FloatTensor(y_val).to(device)
        X_test_tensor = torch.FloatTensor(X_test).to(device)
        y_test_tensor = torch.FloatTensor(y_test).to(device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # åˆ›å»ºæ¨¡å‹
        model = BaselineModel(X.shape[2], self.sequence_length).to(device)
        
        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"   å‚æ•°é‡: {total_params:,}")
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒæ¨¡å‹
        model.train()
        start_time = time.time()
        train_losses = []
        val_losses = []
        
        best_val_loss = float('inf')
        patience = 10
        patience_counter = 0
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            batch_count = 0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
            
            avg_train_loss = epoch_loss / batch_count
            train_losses.append(avg_train_loss)
            
            # éªŒè¯
            model.eval()
            with torch.no_grad():
                val_pred = model(X_val_tensor)
                val_loss = criterion(val_pred.squeeze(), y_val_tensor).item()
                val_losses.append(val_loss)
            model.train()
            
            # æ—©åœæœºåˆ¶
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"   æ—©åœäºç¬¬ {epoch+1} è½®")
                    break
            
            if (epoch + 1) % 10 == 0:
                print(f"   Epoch {epoch+1}/{epochs}, è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, éªŒè¯æŸå¤±: {val_loss:.6f}")
        
        training_time = time.time() - start_time
        print(f"   è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.1f}ç§’")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(best_model_state)
        
        # æœ€ç»ˆè¯„ä¼°
        model.eval()
        with torch.no_grad():
            val_pred = model(X_val_tensor).cpu().numpy().squeeze()
            test_pred = model(X_test_tensor).cpu().numpy().squeeze()
        
        # é€†å˜æ¢åˆ°åŸå§‹å°ºåº¦
        y_val_orig = np.expm1(y_val) - 1.0
        y_test_orig = np.expm1(y_test) - 1.0
        val_pred_orig = np.expm1(val_pred) - 1.0
        test_pred_orig = np.expm1(test_pred) - 1.0
        
        # è®¡ç®—æŒ‡æ ‡
        val_rmse = np.sqrt(mean_squared_error(y_val_orig, val_pred_orig))
        test_rmse = np.sqrt(mean_squared_error(y_test_orig, test_pred_orig))
        val_r2 = r2_score(y_val_orig, val_pred_orig)
        test_r2 = r2_score(y_test_orig, test_pred_orig)
        val_mae = mean_absolute_error(y_val_orig, val_pred_orig)
        test_mae = mean_absolute_error(y_test_orig, test_pred_orig)
        
        # è®¡ç®—MAPE
        val_mape = np.mean(np.abs((y_val_orig - val_pred_orig) / np.maximum(y_val_orig, 1e-8))) * 100
        test_mape = np.mean(np.abs((y_test_orig - test_pred_orig) / np.maximum(y_test_orig, 1e-8))) * 100
        
        # ç»“æœå­—å…¸
        results = {
            'data_size_name': data_size_name,
            'sample_count': len(X),
            'train_samples': len(X_train),
            'val_samples': len(X_val),
            'test_samples': len(X_test),
            'total_params': total_params,
            'training_time': training_time,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'epochs_trained': len(train_losses),
            'val_rmse': val_rmse,
            'val_r2': val_r2,
            'val_mae': val_mae,
            'val_mape': val_mape,
            'test_rmse': test_rmse,
            'test_r2': test_r2,
            'test_mae': test_mae,
            'test_mape': test_mape,
            'y_val_true': y_val_orig,
            'y_val_pred': val_pred_orig,
            'y_test_true': y_test_orig,
            'y_test_pred': test_pred_orig
        }
        
        print(f"   éªŒè¯é›† - RMSE: {val_rmse:.4f}, RÂ²: {val_r2:.4f}, MAE: {val_mae:.4f}")
        print(f"   æµ‹è¯•é›† - RMSE: {test_rmse:.4f}, RÂ²: {test_r2:.4f}, MAE: {test_mae:.4f}")
        
        return results
    
    def run_data_scaling_experiment(self, grid_sizes=None, epochs=30, batch_size=32):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®é‡å¯¹æ¯”å®éªŒ
        
        Args:
            grid_sizes: è¦æµ‹è¯•çš„ç½‘æ ¼æ•°é‡åˆ—è¡¨
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        if grid_sizes is None:
            # é»˜è®¤æµ‹è¯•ä¸åŒçš„ç½‘æ ¼æ•°é‡ï¼ˆå¯¹åº”ä¸åŒçš„æ•°æ®é‡ï¼‰
            grid_sizes = [50, 100, 150, 200, 250, 300, 400, 500, 600, 800]
        
        print("ğŸ“Š å¼€å§‹æ•°æ®é‡å¯¹æ¯”å®éªŒ")
        print("=" * 80)
        print(f"å®éªŒé…ç½®:")
        print(f"  åºåˆ—é•¿åº¦: {self.sequence_length}")
        print(f"  è®­ç»ƒè½®æ•°: {epochs}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  æµ‹è¯•ç½‘æ ¼æ•°é‡: {grid_sizes}")
        print("=" * 80)
        
        # é€ä¸ªæµ‹è¯•ä¸åŒæ•°æ®é‡
        for i, max_grids in enumerate(grid_sizes, 1):
            print(f"\nğŸ“Š å®éªŒ {i}/{len(grid_sizes)}: æœ€å¤§ç½‘æ ¼æ•° = {max_grids}")
            print("-" * 60)
            
            try:
                # åŠ è½½æ•°æ®
                X, y = self.load_and_prepare_data_with_grids(max_grids)
                
                if X is None:
                    continue
                
                # è®­ç»ƒæ¨¡å‹
                data_size_name = f"ç½‘æ ¼æ•°_{max_grids}"
                results = self.train_single_model(
                    X, y, data_size_name,
                    epochs=epochs,
                    batch_size=batch_size
                )
                
                # å­˜å‚¨ç»“æœ
                self.results[max_grids] = results
                print(f"âœ… {data_size_name} å®éªŒå®Œæˆ")
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡è®­ç»ƒæ—¶é—´
                time_per_sample = results['training_time'] / results['train_samples']
                print(f"   å¹³å‡è®­ç»ƒæ—¶é—´: {time_per_sample*1000:.2f} æ¯«ç§’/æ ·æœ¬")
                
            except Exception as e:
                print(f"âŒ ç½‘æ ¼æ•° {max_grids} å®éªŒå¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ¯ æ•°æ®é‡å¯¹æ¯”å®éªŒå®Œæˆ! æˆåŠŸå®Œæˆ {len(self.results)} ä¸ªå®éªŒ")
        return self.results
    
    def create_comparison_report(self):
        """åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯ä¾›å¯¹æ¯”")
            return None
        
        print("\nğŸ“Š åˆ›å»ºæ•°æ®é‡å¯¹æ¯”æŠ¥å‘Š...")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        for max_grids, results in self.results.items():
            comparison_data.append({
                'ç½‘æ ¼æ•°': max_grids,
                'æ ·æœ¬æ€»æ•°': results['sample_count'],
                'è®­ç»ƒæ ·æœ¬æ•°': results['train_samples'],
                'è®­ç»ƒæ—¶é—´(ç§’)': results['training_time'],
                'è®­ç»ƒè½®æ•°': results['epochs_trained'],
                'æ—¶é—´/æ ·æœ¬(ms)': (results['training_time'] / results['train_samples']) * 1000,
                'éªŒè¯_RMSE': results['val_rmse'],
                'éªŒè¯_RÂ²': results['val_r2'],
                'éªŒè¯_MAE': results['val_mae'],
                'éªŒè¯_MAPE': results['val_mape'],
                'æµ‹è¯•_RMSE': results['test_rmse'],
                'æµ‹è¯•_RÂ²': results['test_r2'],
                'æµ‹è¯•_MAE': results['test_mae'],
                'æµ‹è¯•_MAPE': results['test_mape']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # æŒ‰ç½‘æ ¼æ•°æ’åº
        comparison_df = comparison_df.sort_values('ç½‘æ ¼æ•°')
        
        return comparison_df
    
    def create_visualizations(self, save_dir='data_scaling_results'):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ï¼ˆè‹±æ–‡ç‰ˆï¼‰"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯ä¾›å¯è§†åŒ–")
            return
        
        print("ğŸ“Š åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # è®¾ç½®è‹±æ–‡å­—ä½“
        plt.rcParams['font.family'] = 'serif'
        plt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif']
        plt.rcParams['axes.unicode_minus'] = False
        
        # å‡†å¤‡æ•°æ®
        grid_numbers = sorted(self.results.keys())
        sample_counts = [self.results[g]['sample_count'] for g in grid_numbers]
        train_samples = [self.results[g]['train_samples'] for g in grid_numbers]
        
        val_r2 = [self.results[g]['val_r2'] for g in grid_numbers]
        test_r2 = [self.results[g]['test_r2'] for g in grid_numbers]
        val_rmse = [self.results[g]['val_rmse'] for g in grid_numbers]
        test_rmse = [self.results[g]['test_rmse'] for g in grid_numbers]
        
        training_times = [self.results[g]['training_time'] for g in grid_numbers]
        time_per_sample = [(self.results[g]['training_time'] / self.results[g]['train_samples']) * 1000 
                          for g in grid_numbers]
        
        # 1. ä¸»è¦æ€§èƒ½æŒ‡æ ‡éšæ•°æ®é‡å˜åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Data Scaling Experiment Results', fontsize=16, fontweight='bold')
        
        # RÂ²éšæ•°æ®é‡å˜åŒ–
        axes[0, 0].plot(sample_counts, val_r2, 'o-', label='Validation', alpha=0.8, linewidth=2, markersize=6)
        axes[0, 0].plot(sample_counts, test_r2, 's-', label='Test', alpha=0.8, linewidth=2, markersize=6)
        axes[0, 0].set_title('RÂ² Score vs Data Size')
        axes[0, 0].set_xlabel('Total Samples')
        axes[0, 0].set_ylabel('RÂ² Score')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # RMSEéšæ•°æ®é‡å˜åŒ–
        axes[0, 1].plot(sample_counts, val_rmse, 'o-', label='Validation', alpha=0.8, linewidth=2, markersize=6)
        axes[0, 1].plot(sample_counts, test_rmse, 's-', label='Test', alpha=0.8, linewidth=2, markersize=6)
        axes[0, 1].set_title('RMSE vs Data Size')
        axes[0, 1].set_xlabel('Total Samples')
        axes[0, 1].set_ylabel('RMSE')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # è®­ç»ƒæ—¶é—´éšæ•°æ®é‡å˜åŒ–
        axes[1, 0].plot(train_samples, training_times, 'o-', color='green', alpha=0.8, linewidth=2, markersize=6)
        axes[1, 0].set_title('Training Time vs Data Size')
        axes[1, 0].set_xlabel('Training Samples')
        axes[1, 0].set_ylabel('Training Time (seconds)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # è®­ç»ƒæ•ˆç‡ï¼ˆæ—¶é—´/æ ·æœ¬ï¼‰
        axes[1, 1].plot(train_samples, time_per_sample, 'o-', color='red', alpha=0.8, linewidth=2, markersize=6)
        axes[1, 1].set_title('Training Efficiency (time/sample)')
        axes[1, 1].set_xlabel('Training Samples')
        axes[1, 1].set_ylabel('Time per Sample (ms)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜ä¸»è¦å¯¹æ¯”å›¾
        main_comparison_path = os.path.join(save_dir, 'data_scaling_comparison.png')
        plt.savefig(main_comparison_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ä¸»è¦å¯¹æ¯”å›¾ä¿å­˜è‡³: {main_comparison_path}")
        
        # 2. æ€§èƒ½æ”¹è¿›åˆ†æå›¾
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('Performance Improvement Analysis', fontsize=14, fontweight='bold')
        
        # RÂ²æ”¹è¿›æ›²çº¿
        if len(val_r2) > 1:
            r2_improvement = [(val_r2[i] - val_r2[0]) for i in range(len(val_r2))]
            axes[0].plot(sample_counts, r2_improvement, 'o-', color='blue', alpha=0.8, linewidth=2, markersize=6)
            axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)
            axes[0].set_title('RÂ² Improvement Relative to Minimum Data')
            axes[0].set_xlabel('Total Samples')
            axes[0].set_ylabel('RÂ² Improvement')
            axes[0].grid(True, alpha=0.3)
        
        # è¾¹é™…æ”¶ç›Šåˆ†æ
        if len(val_r2) > 1:
            marginal_gains = [0] + [val_r2[i] - val_r2[i-1] for i in range(1, len(val_r2))]
            axes[1].bar(range(len(sample_counts)), marginal_gains, alpha=0.7, color='orange')
            axes[1].set_title('RÂ² Marginal Gains')
            axes[1].set_xlabel('Experiment Index')
            axes[1].set_ylabel('RÂ² Marginal Improvement')
            axes[1].set_xticks(range(len(sample_counts)))
            axes[1].set_xticklabels([f'{sc//1000}K' for sc in sample_counts], rotation=45)
            axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜æ”¹è¿›åˆ†æå›¾
        improvement_path = os.path.join(save_dir, 'performance_improvement_analysis.png')
        plt.savefig(improvement_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š æ”¹è¿›åˆ†æå›¾ä¿å­˜è‡³: {improvement_path}")
        
        print(f"ğŸ“Š æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {save_dir}/")
    
    def save_results(self, save_dir='data_scaling_results'):
        """ä¿å­˜å®éªŒç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯ä¿å­˜")
            return
        
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
        comparison_df = self.create_comparison_report()
        if comparison_df is not None:
            comparison_path = os.path.join(save_dir, 'data_scaling_comparison.csv')
            comparison_df.to_csv(comparison_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š å¯¹æ¯”æŠ¥å‘Š: {comparison_path}")
        
        # 2. ä¿å­˜è¯¦ç»†ç»“æœ
        summary_data = {
            'experiment_info': {
                'name': 'milano_data_scaling_experiment',
                'description': 'Milanoç½‘ç»œæµé‡é¢„æµ‹æ•°æ®é‡å¯¹æ¯”å®éªŒ',
                'datetime': datetime.now().isoformat(),
                'sequence_length': self.sequence_length,
                'device': str(device)
            },
            'grid_sizes_tested': list(self.results.keys()),
            'results_summary': {}
        }
        
        # 3. ä¿å­˜æ¯ä¸ªå®éªŒçš„è¯¦ç»†ç»“æœ
        for grid_size, results in self.results.items():
            # ä¿å­˜é¢„æµ‹ç»“æœ
            predictions_df = pd.DataFrame({
                'val_true': results['y_val_true'],
                'val_pred': results['y_val_pred'],
                'val_error': results['y_val_pred'] - results['y_val_true'],
                'test_true': results['y_test_true'][:len(results['y_val_true'])],
                'test_pred': results['y_test_pred'][:len(results['y_val_true'])],
                'test_error': (results['y_test_pred'] - results['y_test_true'])[:len(results['y_val_true'])]
            })
            pred_path = os.path.join(save_dir, f'grid_{grid_size}_predictions.csv')
            predictions_df.to_csv(pred_path, index=False)
            
            # æ·»åŠ åˆ°æ±‡æ€»æ•°æ®
            summary_data['results_summary'][f'grid_{grid_size}'] = {
                'grid_size': grid_size,
                'sample_count': int(results['sample_count']),
                'train_samples': int(results['train_samples']),
                'training_time': float(results['training_time']),
                'epochs_trained': int(results['epochs_trained']),
                'validation_metrics': {
                    'rmse': float(results['val_rmse']),
                    'r2': float(results['val_r2']),
                    'mae': float(results['val_mae']),
                    'mape': float(results['val_mape'])
                },
                'test_metrics': {
                    'rmse': float(results['test_rmse']),
                    'r2': float(results['test_r2']),
                    'mae': float(results['test_mae']),
                    'mape': float(results['test_mape'])
                }
            }
        
        # 4. ä¿å­˜æ±‡æ€»JSON
        summary_path = os.path.join(save_dir, 'data_scaling_summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ˆ æ±‡æ€»ç»“æœ: {summary_path}")
        
        print(f"ğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {save_dir}/")
        
        return save_dir


def print_final_summary(results, comparison_df, save_dir, total_time):
    """æ‰“å°æœ€ç»ˆæ€»ç»“"""
    print("\n" + "=" * 100)
    print("ğŸ‰ æ•°æ®é‡å¯¹æ¯”å®éªŒå®Œæˆ!")
    print("=" * 100)
    print(f"â±ï¸  æ€»å®éªŒæ—¶é—´: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
    
    if comparison_df is not None and len(comparison_df) > 0:
        print("ğŸ“Š å®éªŒç»“æœæ±‡æ€»:")
        print("-" * 80)
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        print("å…³é”®å‘ç°:")
        
        # æœ€ä½³æ€§èƒ½
        best_r2_idx = comparison_df['éªŒè¯_RÂ²'].idxmax()
        best_result = comparison_df.iloc[best_r2_idx]
        print(f"ğŸ† æœ€ä½³éªŒè¯RÂ²: {best_result['éªŒè¯_RÂ²']:.4f} (ç½‘æ ¼æ•°: {best_result['ç½‘æ ¼æ•°']}, æ ·æœ¬æ•°: {best_result['æ ·æœ¬æ€»æ•°']:,})")
        
        # æ€§èƒ½è¶‹åŠ¿
        first_r2 = comparison_df.iloc[0]['éªŒè¯_RÂ²']
        last_r2 = comparison_df.iloc[-1]['éªŒè¯_RÂ²']
        r2_improvement = last_r2 - first_r2
        print(f"ğŸ“ˆ RÂ²æ€»ä½“æ”¹è¿›: {r2_improvement:.4f} ({first_r2:.4f} â†’ {last_r2:.4f})")
        
        # æ•ˆç‡åˆ†æ
        min_time_per_sample = comparison_df['æ—¶é—´/æ ·æœ¬(ms)'].min()
        max_time_per_sample = comparison_df['æ—¶é—´/æ ·æœ¬(ms)'].max()
        print(f"â±ï¸  è®­ç»ƒæ•ˆç‡èŒƒå›´: {min_time_per_sample:.2f} - {max_time_per_sample:.2f} æ¯«ç§’/æ ·æœ¬")
        
        # æ ·æœ¬æ•°é‡èŒƒå›´
        min_samples = comparison_df['æ ·æœ¬æ€»æ•°'].min()
        max_samples = comparison_df['æ ·æœ¬æ€»æ•°'].max()
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡èŒƒå›´: {min_samples:,} - {max_samples:,}")
        
        print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
        
        # æ‰¾åˆ°æ€§èƒ½é¥±å’Œç‚¹
        r2_scores = comparison_df['éªŒè¯_RÂ²'].values
        if len(r2_scores) > 3:
            # è®¡ç®—æ”¹è¿›çš„è¾¹é™…æ•ˆç›Š
            marginal_gains = [r2_scores[i] - r2_scores[i-1] for i in range(1, len(r2_scores))]
            avg_marginal_gain = np.mean(marginal_gains)
            
            # æ‰¾åˆ°è¾¹é™…æ”¶ç›Šä½äºå¹³å‡å€¼çš„ç‚¹
            saturation_points = [i for i, gain in enumerate(marginal_gains) if gain < avg_marginal_gain * 0.5]
            if saturation_points:
                saturation_idx = saturation_points[0] + 1  # +1å› ä¸ºmarginal_gainsæ¯”åŸæ•°ç»„å°‘1ä¸ªå…ƒç´ 
                saturation_result = comparison_df.iloc[saturation_idx]
                print(f"   ğŸ¯ æ€§èƒ½é¥±å’Œç‚¹: çº¦ {saturation_result['æ ·æœ¬æ€»æ•°']:,} æ ·æœ¬ (ç½‘æ ¼æ•°: {saturation_result['ç½‘æ ¼æ•°']})")
                print(f"      åœ¨æ­¤ç‚¹RÂ²: {saturation_result['éªŒè¯_RÂ²']:.4f}")
        
        # æ•ˆç‡å»ºè®®
        efficiency_scores = comparison_df['éªŒè¯_RÂ²'] / comparison_df['æ—¶é—´/æ ·æœ¬(ms)']
        best_efficiency_idx = efficiency_scores.idxmax()
        efficient_result = comparison_df.iloc[best_efficiency_idx]
        print(f"   âš¡ æœ€é«˜æ•ˆé…ç½®: {efficient_result['æ ·æœ¬æ€»æ•°']:,} æ ·æœ¬ (ç½‘æ ¼æ•°: {efficient_result['ç½‘æ ¼æ•°']})")
        print(f"      RÂ²: {efficient_result['éªŒè¯_RÂ²']:.4f}, æ•ˆç‡: {efficiency_scores.iloc[best_efficiency_idx]:.6f}")
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"   ğŸ“Š å¯¹æ¯”å›¾è¡¨: {save_dir}/data_scaling_comparison.png")
    print(f"   ğŸ“ˆ æ”¹è¿›åˆ†æ: {save_dir}/performance_improvement_analysis.png")
    print(f"   ğŸ“‹ å¯¹æ¯”æ•°æ®: {save_dir}/data_scaling_comparison.csv")
    print(f"   ğŸ’¾ è¯¦ç»†ç»“æœ: {save_dir}/data_scaling_summary.json")
    
    print("=" * 100)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š Milanoç½‘ç»œæµé‡é¢„æµ‹ - æ•°æ®é‡å¯¹æ¯”å®éªŒ")
    print("=" * 80)
    print("ç ”ç©¶é—®é¢˜: æœ€ä½³è¾“å…¥æ•°æ®é‡æ˜¯å¤šå°‘ï¼Ÿ")
    print("å®éªŒæ–¹æ³•: ä½¿ç”¨ä¸åŒæ•°é‡çš„ç½‘æ ¼æ¥æ§åˆ¶æ•°æ®é‡ï¼Œè§‚å¯Ÿæ¨¡å‹æ€§èƒ½å˜åŒ–")
    print("=" * 80)
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = DataScalingExperiment(sequence_length=8)
    
    # å®šä¹‰è¦æµ‹è¯•çš„ç½‘æ ¼æ•°é‡ï¼ˆå¯¹åº”ä¸åŒæ•°æ®é‡ï¼‰
    grid_sizes = [50, 100, 150, 200, 250, 300, 400, 500, 600, 800]
    
    print(f"å°†æµ‹è¯•çš„ç½‘æ ¼æ•°é‡: {grid_sizes}")
    print("é¢„è®¡æ¯ä¸ªå®éªŒéœ€è¦ 3-5 åˆ†é’Ÿ...")
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
    response = input("\næ˜¯å¦å¼€å§‹å®éªŒ? (y/n): ").lower().strip()
    if response not in ['y', 'yes', 'æ˜¯', '']:
        print("âŒ å®éªŒå·²å–æ¶ˆ")
        return
    
    # è¿è¡Œå®éªŒ
    print(f"\nğŸš€ å¼€å§‹æ•°æ®é‡å¯¹æ¯”å®éªŒ...")
    total_start_time = time.time()
    
    try:
        results = experiment.run_data_scaling_experiment(
            grid_sizes=grid_sizes,
            epochs=30,
            batch_size=32
        )
        
        if not results:
            print("âŒ å®éªŒå¤±è´¥!")
            return
        
        total_time = time.time() - total_start_time
        
        # ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ–
        comparison_df = experiment.create_comparison_report()
        experiment.create_visualizations()
        save_dir = experiment.save_results()
        
        # æ‰“å°æœ€ç»ˆæ€»ç»“
        print_final_summary(results, comparison_df, save_dir, total_time)
        
    except KeyboardInterrupt:
        print("\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
