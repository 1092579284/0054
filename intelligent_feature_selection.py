"""
Intelligent Feature Selection for Milano Traffic Prediction
æ™ºèƒ½ç‰¹å¾é€‰æ‹©å·¥å…· - é€šè¿‡å¤šæ¬¡è®­ç»ƒæ‰¾åˆ°æœ€ä½³ç‰¹å¾ç»„åˆ
"""
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from itertools import combinations
import json
import time
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class FastTrainingModel(nn.Module):
    """å¿«é€Ÿè®­ç»ƒçš„ç®€åŒ–æ¨¡å‹"""
    def __init__(self, input_size, sequence_length):
        super(FastTrainingModel, self).__init__()
        
        self.gru = nn.GRU(input_size, 64, batch_first=True, bidirectional=True, dropout=0.2)
        self.lstm = nn.LSTM(128, 32, batch_first=True, dropout=0.2)
        self.dense = nn.Linear(32, 64)
        self.dropout = nn.Dropout(0.2)
        self.output = nn.Linear(64, 1)
        
    def forward(self, x):
        x, _ = self.gru(x)
        x, _ = self.lstm(x)
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        x = torch.relu(self.dense(x))
        x = self.dropout(x)
        x = self.output(x)
        return x

class IntelligentFeatureSelector:
    """æ™ºèƒ½ç‰¹å¾é€‰æ‹©å™¨"""
    
    def __init__(self, data_path, max_grids=500, sequence_length=8):
        self.data_path = data_path
        self.max_grids = max_grids
        self.sequence_length = sequence_length
        self.results_history = []
        self.feature_importance_scores = defaultdict(list)
        
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        self.load_and_prepare_data()
        
    def load_and_prepare_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("ğŸ“Š Loading and preparing data...")
        
        df = pd.read_csv(self.data_path)
        df['time_interval'] = pd.to_datetime(df['time_interval'])
        df = df.sort_values(['square_id', 'time_interval'])
        
        # é€‰æ‹©é«˜è´¨é‡ç½‘æ ¼ï¼ˆå‡å°‘æ•°é‡ä»¥åŠ é€Ÿå®éªŒï¼‰
        grid_stats = df.groupby('square_id')['internet_traffic'].agg([
            'mean', 'count', 'std', 'min', 'max'
        ]).reset_index()
        
        quality_grids = grid_stats[
            (grid_stats['count'] == 48) &
            (grid_stats['std'] > 3) &
            (grid_stats['mean'] > 5) &
            (grid_stats['max'] > 20)
        ].sort_values('mean', ascending=False)
        
        selected_grids = quality_grids['square_id'].head(self.max_grids).tolist()
        self.data = df[df['square_id'].isin(selected_grids)].copy()
        
        # åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„ç‰¹å¾
        try:
            self.create_all_features()
        except Exception as e:
            print(f"âš ï¸  Complex feature creation failed: {e}")
            print("ğŸ”„ Falling back to basic features...")
            self.create_basic_features_only()
        
        print(f"âœ… Data prepared: {len(self.data):,} records, {len(selected_grids)} grids")
        
    def create_all_features(self):
        """åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„ç‰¹å¾"""
        print("ğŸ”§ Creating comprehensive feature set...")
        
        data = self.data.copy()  # ä½¿ç”¨å‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        
        # åŸºç¡€æ—¶é—´ç‰¹å¾
        data['hour'] = data['time_interval'].dt.hour
        data['day_of_week'] = data['time_interval'].dt.dayofweek
        data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
        data['is_workday'] = (data['day_of_week'] < 5).astype(int)
        
        # å‘¨æœŸæ€§ç¼–ç 
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
        data['dow_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
        data['dow_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)
        
        # æ—¶æ®µåˆ†ç±»
        data['is_morning'] = ((data['hour'] >= 6) & (data['hour'] < 12)).astype(int)
        data['is_afternoon'] = ((data['hour'] >= 12) & (data['hour'] < 18)).astype(int)
        data['is_evening'] = ((data['hour'] >= 18) & (data['hour'] < 24)).astype(int)
        data['is_night'] = ((data['hour'] >= 0) & (data['hour'] < 6)).astype(int)
        data['is_peak_hour'] = ((data['hour'] >= 8) & (data['hour'] <= 10) | 
                               (data['hour'] >= 17) & (data['hour'] <= 19)).astype(int)
        
        # æ»åç‰¹å¾
        for lag in range(1, 7):  # lag1 åˆ° lag6
            data[f'traffic_lag{lag}'] = data.groupby('square_id')['internet_traffic'].shift(lag)
        
        # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        for window in [3, 6, 12]:
            data[f'traffic_rolling_mean_{window}'] = data.groupby('square_id')['internet_traffic'].rolling(
                window=window, min_periods=1).mean().reset_index(0, drop=True)
            data[f'traffic_rolling_std_{window}'] = data.groupby('square_id')['internet_traffic'].rolling(
                window=window, min_periods=1).std().reset_index(0, drop=True)
            data[f'traffic_rolling_max_{window}'] = data.groupby('square_id')['internet_traffic'].rolling(
                window=window, min_periods=1).max().reset_index(0, drop=True)
            data[f'traffic_rolling_min_{window}'] = data.groupby('square_id')['internet_traffic'].rolling(
                window=window, min_periods=1).min().reset_index(0, drop=True)
        
        # å·®åˆ†ç‰¹å¾
        for diff in [1, 2]:
            data[f'traffic_diff{diff}'] = data.groupby('square_id')['internet_traffic'].diff(diff)
        
        # å˜åŒ–ç‡ç‰¹å¾
        data['traffic_pct_change'] = data.groupby('square_id')['internet_traffic'].pct_change()
        data['traffic_log_return'] = data.groupby('square_id')['internet_traffic'].transform(
            lambda x: np.log(x / x.shift(1)))
        
        # ç½‘æ ¼ç‰¹å¾
        grid_mapping = {grid_id: idx for idx, grid_id in enumerate(self.data['square_id'].unique())}
        data['grid_feature'] = data['square_id'].map(grid_mapping)
        data['grid_normalized'] = data['grid_feature'] / len(grid_mapping)
        
        # å¡«å……ç¼ºå¤±å€¼
        try:
            self.data = data.fillna(method='ffill').fillna(method='bfill').fillna(0)
        except Exception as e:
            print(f"âš ï¸  Warning: Using alternative fillna method due to: {e}")
            # ä½¿ç”¨æ–°çš„pandasè¯­æ³•
            self.data = data.ffill().bfill().fillna(0)
        
        print(f"âœ… Created {len(self.data.columns) - 3} features")  # å‡å»æ—¶é—´ã€square_idã€internet_traffic
    
    def create_basic_features_only(self):
        """åˆ›å»ºåŸºç¡€ç‰¹å¾é›†åˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        print("ğŸ”§ Creating basic feature set (fallback)...")
        
        data = self.data.copy()
        
        # åŸºç¡€æ—¶é—´ç‰¹å¾
        data['hour'] = data['time_interval'].dt.hour
        data['day_of_week'] = data['time_interval'].dt.dayofweek
        data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
        
        # å‘¨æœŸæ€§ç¼–ç 
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
        
        # æ»åç‰¹å¾
        for lag in range(1, 4):
            data[f'traffic_lag{lag}'] = data.groupby('square_id')['internet_traffic'].shift(lag)
        
        # ç®€å•æ»šåŠ¨å‡å€¼
        data['rolling_mean_3'] = data.groupby('square_id')['internet_traffic'].rolling(
            window=3, min_periods=1).mean().reset_index(0, drop=True)
        
        # ç½‘æ ¼ç‰¹å¾
        grid_mapping = {grid_id: idx for idx, grid_id in enumerate(self.data['square_id'].unique())}
        data['grid_feature'] = data['square_id'].map(grid_mapping)
        
        # å¡«å……ç¼ºå¤±å€¼
        try:
            self.data = data.fillna(method='ffill').fillna(method='bfill').fillna(0)
        except:
            self.data = data.ffill().bfill().fillna(0)
        
        print(f"âœ… Created {len(self.data.columns) - 3} basic features")
        
    def define_feature_groups(self):
        """å®šä¹‰ç‰¹å¾åˆ†ç»„"""
        return {
            'core': ['internet_traffic', 'grid_feature'],
            'basic_time': ['hour', 'day_of_week', 'is_weekend'],
            'time_cyclical': ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos'],
            'time_categories': ['is_morning', 'is_afternoon', 'is_evening', 'is_night', 'is_peak_hour', 'is_workday'],
            'lag_basic': ['traffic_lag1', 'traffic_lag2', 'traffic_lag3'],
            'lag_extended': ['traffic_lag4', 'traffic_lag5', 'traffic_lag6'],
            'rolling_mean': ['traffic_rolling_mean_3', 'traffic_rolling_mean_6', 'traffic_rolling_mean_12'],
            'rolling_stats': ['traffic_rolling_std_3', 'traffic_rolling_std_6', 'traffic_rolling_std_12',
                             'traffic_rolling_max_3', 'traffic_rolling_min_3'],
            'differences': ['traffic_diff1', 'traffic_diff2'],
            'changes': ['traffic_pct_change', 'traffic_log_return'],
            'grid_features': ['grid_normalized']
        }
    
    def create_sequences(self, features):
        """åˆ›å»ºåºåˆ—æ•°æ®"""
        X_all, y_all = [], []
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨
        available_features = [f for f in features if f in self.data.columns]
        if len(available_features) != len(features):
            missing = set(features) - set(available_features)
            print(f"âš ï¸  Missing features: {missing}")
            return None, None
        
        for grid_id in self.data['square_id'].unique():
            grid_data = self.data[self.data['square_id'] == grid_id].sort_values('time_interval')
            
            if len(grid_data) < self.sequence_length + 1:
                continue
                
            feature_data = grid_data[available_features].values
            target_data = grid_data['internet_traffic'].values
            
            for i in range(len(feature_data) - self.sequence_length):
                X_all.append(feature_data[i:(i + self.sequence_length)])
                y_all.append(target_data[i + self.sequence_length])
        
        if len(X_all) == 0:
            return None, None
            
        X = np.array(X_all)
        y = np.array(y_all)
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_reshaped = X.reshape(-1, X.shape[-1])
        X_scaled = scaler.fit_transform(X_reshaped)
        X_scaled = X_scaled.reshape(X.shape)
        
        # ç›®æ ‡å˜æ¢
        y_transformed = np.log1p(y + 1.0)
        
        return X_scaled, y_transformed
    
    def train_and_evaluate(self, features, epochs=30, batch_size=32):
        """è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
        X, y = self.create_sequences(features)
        
        if X is None or len(X) < 100:
            return None
        
        # æ•°æ®åˆ†å‰²
        train_size = int(0.7 * len(X))
        val_size = int(0.15 * len(X))
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size + val_size]
        y_val = y[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]
        y_test = y[train_size + val_size:]
        
        # è½¬æ¢ä¸ºTensor
        X_train_tensor = torch.FloatTensor(X_train).to(device)
        y_train_tensor = torch.FloatTensor(y_train).to(device)
        X_val_tensor = torch.FloatTensor(X_val).to(device)
        y_val_tensor = torch.FloatTensor(y_val).to(device)
        X_test_tensor = torch.FloatTensor(X_test).to(device)
        y_test_tensor = torch.FloatTensor(y_test).to(device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # åˆ›å»ºæ¨¡å‹
        model = FastTrainingModel(X.shape[2], self.sequence_length).to(device)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒæ¨¡å‹
        model.train()
        for epoch in range(epochs):
            epoch_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
        
        # è¯„ä¼°
        model.eval()
        with torch.no_grad():
            val_pred = model(X_val_tensor).cpu().numpy().squeeze()
            test_pred = model(X_test_tensor).cpu().numpy().squeeze()
        
        # é€†å˜æ¢
        y_val_orig = np.expm1(y_val) - 1.0
        y_test_orig = np.expm1(y_test) - 1.0
        val_pred_orig = np.expm1(val_pred) - 1.0
        test_pred_orig = np.expm1(test_pred) - 1.0
        
        # è®¡ç®—æŒ‡æ ‡
        val_rmse = np.sqrt(mean_squared_error(y_val_orig, val_pred_orig))
        test_rmse = np.sqrt(mean_squared_error(y_test_orig, test_pred_orig))
        val_r2 = r2_score(y_val_orig, val_pred_orig)
        test_r2 = r2_score(y_test_orig, test_pred_orig)
        
        return {
            'val_rmse': val_rmse,
            'test_rmse': test_rmse,
            'val_r2': val_r2,
            'test_r2': test_r2,
            'feature_count': len(features)
        }
    
    def forward_selection(self, max_features=15):
        """å‰å‘ç‰¹å¾é€‰æ‹©"""
        print("ğŸ” Starting Forward Feature Selection...")
        
        feature_groups = self.define_feature_groups()
        
        # ä»æ ¸å¿ƒç‰¹å¾å¼€å§‹
        selected_features = feature_groups['core'].copy()
        remaining_groups = {k: v for k, v in feature_groups.items() if k != 'core'}
        
        best_score = float('inf')
        selection_history = []
        
        print(f"Starting with core features: {selected_features}")
        
        # è¯„ä¼°åˆå§‹ç‰¹å¾é›†
        initial_result = self.train_and_evaluate(selected_features)
        if initial_result:
            best_score = initial_result['val_rmse']
            selection_history.append({
                'features': selected_features.copy(),
                'group_added': 'core',
                'result': initial_result
            })
            print(f"Initial RMSE: {best_score:.4f}")
        
        # é€æ­¥æ·»åŠ ç‰¹å¾ç»„
        for step in range(max_features):
            if not remaining_groups:
                break
                
            best_group = None
            best_improvement = 0
            best_result = None
            
            print(f"\n--- Step {step + 1}: Testing {len(remaining_groups)} feature groups ---")
            
            for group_name, group_features in remaining_groups.items():
                test_features = selected_features + group_features
                
                print(f"Testing group '{group_name}': {group_features}")
                
                result = self.train_and_evaluate(test_features)
                
                if result and result['val_rmse'] < best_score:
                    improvement = best_score - result['val_rmse']
                    if improvement > best_improvement:
                        best_improvement = improvement
                        best_group = group_name
                        best_result = result
                
                print(f"  RMSE: {result['val_rmse']:.4f} (improvement: {best_score - result['val_rmse']:.4f})")
            
            # æ·»åŠ æœ€ä½³ç‰¹å¾ç»„
            if best_group:
                selected_features.extend(remaining_groups[best_group])
                best_score = best_result['val_rmse']
                
                selection_history.append({
                    'features': selected_features.copy(),
                    'group_added': best_group,
                    'result': best_result
                })
                
                print(f"âœ… Added group '{best_group}': RMSE improved by {best_improvement:.4f}")
                print(f"Current features ({len(selected_features)}): {selected_features}")
                
                del remaining_groups[best_group]
            else:
                print("âŒ No improvement found, stopping selection")
                break
        
        return selection_history
    
    def random_search(self, num_trials=50):
        """éšæœºæœç´¢ç‰¹å¾ç»„åˆ"""
        print("ğŸ² Starting Random Search...")
        
        feature_groups = self.define_feature_groups()
        all_features = []
        for group_features in feature_groups.values():
            all_features.extend(group_features)
        
        # ç§»é™¤é‡å¤ç‰¹å¾
        all_features = list(set(all_features))
        
        results = []
        
        for trial in range(num_trials):
            # éšæœºé€‰æ‹©ç‰¹å¾æ•°é‡ï¼ˆ6-20ï¼‰
            num_features = np.random.randint(6, min(21, len(all_features)))
            
            # éšæœºé€‰æ‹©ç‰¹å¾
            selected_features = np.random.choice(all_features, num_features, replace=False).tolist()
            
            # ç¡®ä¿åŒ…å«æ ¸å¿ƒç‰¹å¾
            core_features = feature_groups['core']
            for feature in core_features:
                if feature not in selected_features:
                    selected_features.append(feature)
            
            print(f"Trial {trial + 1}/{num_trials}: Testing {len(selected_features)} features")
            
            result = self.train_and_evaluate(selected_features)
            
            if result:
                results.append({
                    'trial': trial + 1,
                    'features': selected_features,
                    'result': result
                })
                print(f"  RMSE: {result['val_rmse']:.4f}, RÂ²: {result['val_r2']:.4f}")
        
        # æŒ‰RMSEæ’åº
        results.sort(key=lambda x: x['result']['val_rmse'])
        
        return results
    
    def exhaustive_group_search(self):
        """ç©·ä¸¾ç‰¹å¾ç»„åˆæœç´¢"""
        print("ğŸ” Starting Exhaustive Group Search...")
        
        feature_groups = self.define_feature_groups()
        core_features = feature_groups['core']
        optional_groups = {k: v for k, v in feature_groups.items() if k != 'core'}
        
        results = []
        total_combinations = 2 ** len(optional_groups)
        
        print(f"Testing {total_combinations} group combinations...")
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç»„åˆ
        group_names = list(optional_groups.keys())
        
        for i in range(total_combinations):
            selected_groups = []
            features = core_features.copy()
            
            # æ ¹æ®äºŒè¿›åˆ¶è¡¨ç¤ºé€‰æ‹©ç‰¹å¾ç»„
            for j, group_name in enumerate(group_names):
                if i & (1 << j):
                    selected_groups.append(group_name)
                    features.extend(optional_groups[group_name])
            
            if len(features) < 4:  # è‡³å°‘4ä¸ªç‰¹å¾
                continue
            
            print(f"Combination {i + 1}/{total_combinations}: Groups {selected_groups}")
            
            result = self.train_and_evaluate(features)
            
            if result:
                results.append({
                    'combination_id': i,
                    'selected_groups': selected_groups,
                    'features': features,
                    'result': result
                })
                print(f"  RMSE: {result['val_rmse']:.4f}, Features: {len(features)}")
        
        # æŒ‰RMSEæ’åº
        results.sort(key=lambda x: x['result']['val_rmse'])
        
        return results
    
    def analyze_results(self, forward_results, random_results, exhaustive_results):
        """åˆ†æå’Œå¯è§†åŒ–ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ“Š FEATURE SELECTION ANALYSIS RESULTS")
        print("="*80)
        
        # æ‰¾å‡ºæ¯ç§æ–¹æ³•çš„æœ€ä½³ç»“æœ
        best_forward = min(forward_results, key=lambda x: x['result']['val_rmse'])
        best_random = min(random_results, key=lambda x: x['result']['val_rmse'])
        best_exhaustive = min(exhaustive_results, key=lambda x: x['result']['val_rmse'])
        
        print("\nğŸ† BEST RESULTS FROM EACH METHOD:")
        print("-" * 50)
        print(f"Forward Selection:")
        print(f"  RMSE: {best_forward['result']['val_rmse']:.4f}")
        print(f"  RÂ²: {best_forward['result']['val_r2']:.4f}")
        print(f"  Features ({len(best_forward['features'])}): {best_forward['features']}")
        
        print(f"\nRandom Search:")
        print(f"  RMSE: {best_random['result']['val_rmse']:.4f}")
        print(f"  RÂ²: {best_random['result']['val_r2']:.4f}")
        print(f"  Features ({len(best_random['features'])}): {best_random['features']}")
        
        print(f"\nExhaustive Search:")
        print(f"  RMSE: {best_exhaustive['result']['val_rmse']:.4f}")
        print(f"  RÂ²: {best_exhaustive['result']['val_r2']:.4f}")
        print(f"  Groups: {best_exhaustive['selected_groups']}")
        print(f"  Features ({len(best_exhaustive['features'])}): {best_exhaustive['features']}")
        
        # æ‰¾å‡ºå…¨å±€æœ€ä½³
        all_best = [
            ('Forward Selection', best_forward),
            ('Random Search', best_random),
            ('Exhaustive Search', best_exhaustive)
        ]
        
        global_best = min(all_best, key=lambda x: x[1]['result']['val_rmse'])
        
        print(f"\nğŸ¥‡ GLOBAL BEST RESULT:")
        print(f"Method: {global_best[0]}")
        print(f"RMSE: {global_best[1]['result']['val_rmse']:.4f}")
        print(f"RÂ²: {global_best[1]['result']['val_r2']:.4f}")
        
        return global_best
    
    def visualize_results(self, forward_results, random_results, exhaustive_results):
        """å¯è§†åŒ–ç»“æœ"""
        plt.figure(figsize=(15, 10))
        
        # 1. å‰å‘é€‰æ‹©è¿›åº¦
        plt.subplot(2, 3, 1)
        forward_rmse = [r['result']['val_rmse'] for r in forward_results]
        forward_features = [len(r['features']) for r in forward_results]
        plt.plot(forward_features, forward_rmse, 'o-', color='blue')
        plt.xlabel('Number of Features')
        plt.ylabel('Validation RMSE')
        plt.title('Forward Selection Progress')
        plt.grid(True, alpha=0.3)
        
        # 2. éšæœºæœç´¢åˆ†å¸ƒ
        plt.subplot(2, 3, 2)
        random_rmse = [r['result']['val_rmse'] for r in random_results]
        random_features = [len(r['features']) for r in random_results]
        plt.scatter(random_features, random_rmse, alpha=0.6, color='orange')
        plt.xlabel('Number of Features')
        plt.ylabel('Validation RMSE')
        plt.title('Random Search Results')
        plt.grid(True, alpha=0.3)
        
        # 3. ç©·ä¸¾æœç´¢ç»“æœ
        plt.subplot(2, 3, 3)
        exhaustive_rmse = [r['result']['val_rmse'] for r in exhaustive_results[:20]]  # åªæ˜¾ç¤ºå‰20
        exhaustive_features = [len(r['features']) for r in exhaustive_results[:20]]
        plt.scatter(exhaustive_features, exhaustive_rmse, alpha=0.8, color='green')
        plt.xlabel('Number of Features')
        plt.ylabel('Validation RMSE')
        plt.title('Exhaustive Search (Top 20)')
        plt.grid(True, alpha=0.3)
        
        # 4. æ–¹æ³•å¯¹æ¯”
        plt.subplot(2, 3, 4)
        methods = ['Forward\nSelection', 'Random\nSearch', 'Exhaustive\nSearch']
        best_rmse = [
            min(forward_results, key=lambda x: x['result']['val_rmse'])['result']['val_rmse'],
            min(random_results, key=lambda x: x['result']['val_rmse'])['result']['val_rmse'],
            min(exhaustive_results, key=lambda x: x['result']['val_rmse'])['result']['val_rmse']
        ]
        bars = plt.bar(methods, best_rmse, color=['blue', 'orange', 'green'], alpha=0.7)
        plt.ylabel('Best RMSE')
        plt.title('Method Comparison')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, best_rmse):
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.001,
                    f'{value:.3f}', ha='center', va='bottom')
        
        # 5. ç‰¹å¾æ•°é‡ vs æ€§èƒ½
        plt.subplot(2, 3, 5)
        all_rmse = random_rmse + [r['result']['val_rmse'] for r in exhaustive_results]
        all_features = random_features + [len(r['features']) for r in exhaustive_results]
        plt.scatter(all_features, all_rmse, alpha=0.5, s=20)
        plt.xlabel('Number of Features')
        plt.ylabel('Validation RMSE')
        plt.title('Feature Count vs Performance')
        plt.grid(True, alpha=0.3)
        
        # 6. RÂ² vs RMSE
        plt.subplot(2, 3, 6)
        all_r2 = [r['result']['val_r2'] for r in random_results + exhaustive_results]
        all_rmse_full = [r['result']['val_rmse'] for r in random_results + exhaustive_results]
        plt.scatter(all_r2, all_rmse_full, alpha=0.5, s=20)
        plt.xlabel('RÂ² Score')
        plt.ylabel('RMSE')
        plt.title('RÂ² vs RMSE')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('feature_selection_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š Visualization saved as 'feature_selection_analysis.png'")
    
    def save_results(self, forward_results, random_results, exhaustive_results, global_best):
        """ä¿å­˜ç»“æœ"""
        results = {
            'forward_selection': forward_results,
            'random_search': random_results,
            'exhaustive_search': exhaustive_results,
            'global_best': {
                'method': global_best[0],
                'features': global_best[1].get('features', []),
                'result': global_best[1]['result']
            },
            'summary': {
                'total_experiments': len(forward_results) + len(random_results) + len(exhaustive_results),
                'best_rmse': global_best[1]['result']['val_rmse'],
                'best_r2': global_best[1]['result']['val_r2'],
                'optimal_feature_count': len(global_best[1].get('features', []))
            }
        }
        
        with open('feature_selection_results.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print("ğŸ’¾ Results saved as 'feature_selection_results.json'")
        
    def run_complete_feature_selection(self):
        """è¿è¡Œå®Œæ•´çš„ç‰¹å¾é€‰æ‹©æµç¨‹"""
        print("ğŸš€ Starting Comprehensive Feature Selection...")
        print(f"Dataset: {len(self.data):,} records")
        print(f"Grids: {self.data['square_id'].nunique()}")
        print(f"Available features: {len(self.data.columns) - 3}")
        
        start_time = time.time()
        
        # 1. å‰å‘é€‰æ‹©
        forward_results = self.forward_selection(max_features=10)
        
        # 2. éšæœºæœç´¢
        random_results = self.random_search(num_trials=30)
        
        # 3. ç©·ä¸¾ç»„åˆæœç´¢
        exhaustive_results = self.exhaustive_group_search()
        
        # 4. åˆ†æç»“æœ
        global_best = self.analyze_results(forward_results, random_results, exhaustive_results)
        
        # 5. å¯è§†åŒ–
        self.visualize_results(forward_results, random_results, exhaustive_results)
        
        # 6. ä¿å­˜ç»“æœ
        self.save_results(forward_results, random_results, exhaustive_results, global_best)
        
        elapsed_time = time.time() - start_time
        print(f"\nâ±ï¸  Total execution time: {elapsed_time:.1f} seconds")
        
        return global_best

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Intelligent Feature Selection for Milano Traffic Prediction")
    print("="*80)
    
    # æ•°æ®è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    data_path = "/root/autodl-tmp/xiaorong0802/data/cleaned_dataset_30/cleaned-sms-call-internet-mi-2013-11-01.csv"
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    import os
    if not os.path.exists(data_path):
        print(f"âŒ Data file not found: {data_path}")
        print("ğŸ’¡ Please check the file path and try again")
        return
    
    print(f"ğŸ“ Using data file: {data_path}")
    
    # åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨
    selector = IntelligentFeatureSelector(
        data_path=data_path,
        max_grids=300,  # å‡å°‘ç½‘æ ¼æ•°é‡ä»¥åŠ é€Ÿå®éªŒ
        sequence_length=8
    )
    
    # è¿è¡Œå®Œæ•´çš„ç‰¹å¾é€‰æ‹©
    best_result = selector.run_complete_feature_selection()
    
    print("\nğŸ‰ Feature selection completed!")
    print(f"ğŸ† Best feature combination found:")
    print(f"   Method: {best_result[0]}")
    print(f"   RMSE: {best_result[1]['result']['val_rmse']:.4f}")
    print(f"   RÂ²: {best_result[1]['result']['val_r2']:.4f}")
    
    if 'features' in best_result[1]:
        print(f"   Features ({len(best_result[1]['features'])}): {best_result[1]['features']}")

if __name__ == "__main__":
    main()